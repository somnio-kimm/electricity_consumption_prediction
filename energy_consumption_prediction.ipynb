{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g28ouXJLra9S"
   },
   "outputs": [],
   "source": [
    "# Department: ESTSOFT\n",
    "# Class: AI Modelling\n",
    "# Category: Machine learning\n",
    "# Title: Energy consumption prediction\n",
    "# Contributors: Jeong Gukho, Jeong Woogun, Kim Hyungeun, Kim Juneon, Kimm Soo Min\n",
    "# Last modified date: 10/04/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UsN07m3ra9U"
   },
   "source": [
    "### **Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dxsIQ6Kura9V",
    "outputId": "61f53f8c-9673-446d-cfda-d9b4e82e3eed"
   },
   "outputs": [],
   "source": [
    "# Library\n",
    "# Time\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# File\n",
    "import warnings\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import chardet\n",
    "import itertools\n",
    "\n",
    "# Numerical & Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import math\n",
    "from typing import List, Callable, Union, Dict, Any, Tuple\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "from sklearn.tree import plot_tree\n",
    "from scipy.optimize import curve_fit\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "\t\t\t\t\t\t\t  ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "\t\t\t\t\t\t\t  BaggingClassifier, BaggingRegressor,\n",
    "\t\t\t\t\t\t\t  GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "\t\t\t\t\t\t\t  AdaBoostClassifier, AdaBoostRegressor,\n",
    "\t\t\t\t\t\t\t  VotingClassifier, VotingRegressor,\n",
    "\t\t\t\t\t\t\t  StackingClassifier, StackingRegressor)\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Neural Network Libraries\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor\n",
    "import huggingface\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import (StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer,\n",
    "\t\t\t\t\t\t\t\t   LabelEncoder, OneHotEncoder, OrdinalEncoder, LabelBinarizer)\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, RFE, SequentialFeatureSelector, VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import SimpleImputer\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Time-Series Analysis\n",
    "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (classification_report, pairwise_distances, silhouette_score,\n",
    "\t\t\t\t\t\t\t roc_curve, auc, roc_auc_score, RocCurveDisplay,\n",
    "\t\t\t\t\t\t\t confusion_matrix, ConfusionMatrixDisplay,\n",
    "\t\t\t\t\t\t\t accuracy_score, recall_score, precision_score, f1_score,\n",
    "\t\t\t\t\t\t\t log_loss, hinge_loss, mean_absolute_error, mean_squared_error, r2_score)\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC, ConfusionMatrix, MeanSquaredError, MeanAbsoluteError, R2Score, MetricCollection\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgMbPSOPra9W"
   },
   "source": [
    "### **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OR16lpVPra9W",
    "outputId": "f893d7ab-ca21-4caf-d578-e1393708c5ad"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/cleaned/energy_consumption.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y5kq3hJira9W"
   },
   "outputs": [],
   "source": [
    "def dual_features_plot(a: str, b: str, c: str):\n",
    "\t'''\n",
    "\tGenerate a dual-axis plot to visualize a and b averaged on c.\n",
    "\n",
    "\tParameters:\n",
    "\ta (str): Column name for data in the DataFrame to compare.\n",
    "\tb (str): Column name for data in the DataFrame to compare.\n",
    "\tc (str): Column name for data in the DataFrame to set average on.\n",
    "\t'''\n",
    "\n",
    "\tif c == 'Month': # Take mean for Month\n",
    "\t\tdata = df.groupby(c)[[a, b]].mean().reset_index()\n",
    "\telif c == 'Year': # Take cumulative sum for Year\n",
    "\t\tdata = df.groupby(c)[[a, b]].sum().reset_index()\n",
    "\telse:\n",
    "\t\traise ValueError(\"Grouping column must be 'Month' or 'Year'\")\n",
    "\n",
    "\t# Axis 1\n",
    "\tfig, ax_1 = plt.subplots(figsize=(12, 8))\n",
    "\tax_1.set_xlabel(f'{c}')\n",
    "\tax_1.set_ylabel(f'{a}')\n",
    "\tax_1.bar(data[c], data[a], color='orange')\n",
    "\tax_1.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "\t# Axis 2\n",
    "\tax_2 = ax_1.twinx()\n",
    "\tax_2.set_ylabel(f'{b}')\n",
    "\tax_2.plot(data[c], data[b], marker='o', color='blue')\n",
    "\tax_2.tick_params(axis='y', labelcolor='blue')\n",
    "\tfor x_val,y_val in zip(data[c],data[b]):\n",
    "\t\tax_2.text(x_val, y_val +(ax_2.get_ylim()[1] - ax_2.get_ylim()[0]) * 0.02,f'{y_val:.2f}',ha='center',va='bottom',fontsize=9,color='black')\n",
    "\n",
    "\tplt.title(f'{a} & {b} from 2016 to 2024')\n",
    "\tplt.grid()\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ecDOseyhra9X"
   },
   "outputs": [],
   "source": [
    "def triple_features_plot(df, a: str, b: str, c: str, d: str):\n",
    "\t'''\n",
    "\tGenerate a triple-axis plot to visualize a, b, c based on c.\n",
    "\n",
    "\tParameters:\n",
    "\ta (str): Column name for data in the DataFrame to compare. (Bar)\n",
    "\tb (str): Column name for data in the DataFrame to compare. (Line)\n",
    "\tc (str): Column name for data in the DataFrame to compare. (Line)\n",
    "\td (str): Column name for data in the DataFrame to set base on.\n",
    "\t'''\n",
    "\n",
    "\tif d == 'Month':\n",
    "\t\tdata = df.groupby(d)[[a, b, c]].mean().reset_index()\n",
    "\telif d == 'Year':\n",
    "\t\tdata = df.groupby(d)[[a, b, c]].sum().reset_index()\n",
    "\telse:\n",
    "\t\traise ValueError(\"Grouping column must be 'Month' or 'Year'\")\n",
    "\n",
    "\tfig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\t# Axis 1\n",
    "\tax1.set_xlabel(d)\n",
    "\tax1.set_ylabel(a, color='orange')\n",
    "\tbars = ax1.bar(data[d], data[a], color='orange', label=a)\n",
    "\tax1.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "\t# Axis 2\n",
    "\tax2 = ax1.twinx()\n",
    "\tax2.set_ylabel(b, color='blue')\n",
    "\tp2, = ax2.plot(data[d], data[b], color='blue', marker='o', label=b)\n",
    "\tax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "\t# Axis 3\n",
    "\tax3 = ax1.twinx()\n",
    "\tax3.spines['right'].set_position(('outward', 60))  # Offset to avoid overlap\n",
    "\tax3.set_ylabel(c, color='green')\n",
    "\tp3, = ax3.plot(data[d], data[c], color='green', marker='s', label=c)\n",
    "\tax3.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "\tfig.suptitle(f'{a}, {b}, and {c} grouped by {d}')\n",
    "\tplt.grid()\n",
    "\tfig.tight_layout()\n",
    "\tfig.legend([bars, p2, p3], [a, b, c], loc='upper left', bbox_to_anchor=(0.07, 0.91))\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HO6fslyqra9X"
   },
   "outputs": [],
   "source": [
    "def one_feature_two_group (a: str, b: str, c: str):\n",
    "\t\"\"\"\n",
    "\tPlot the mean of a single feature 'c' grouped by two categorical variables 'a' and 'b'.\n",
    "\n",
    "\tParameters:\n",
    "\ta (str): Name of the main category column in df (e.g., a higher-level group like 'Region').\n",
    "\tb (str): Name of the subcategory column in df (e.g., a more granular group like 'City').\n",
    "\tc (str): Name of the feature / metric column whose mean is to be plotted (e.g., 'Sales').\n",
    "\t\"\"\"\n",
    "\tdf_mean = df.groupby([f'{a}',f'{b}'])[c].mean()\n",
    "\tunstacked_df = df_mean.unstack(level=f'{a}')\n",
    "\tplt.figure(figsize=(12, 8))\n",
    "\tunstacked_df.plot(kind='line',marker='o',ax=plt.gca())\n",
    "\n",
    "\n",
    "\tplt.xlabel(f'{b}')\n",
    "\tplt.ylabel(f'{c}')\n",
    "\tplt.grid()\n",
    "\tplt.title(f'{c} by {a} and {b}')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OpPY5f4ora9X",
    "outputId": "d68db958-961a-47b7-9df9-cec64891b86f"
   },
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "df_numeric=df.select_dtypes(include=['number'])\n",
    "df_corr=df_numeric.corr()\n",
    "# df_corr_find = df_corr.copy()\n",
    "# low_cols = df_corr_find[abs(df_corr_find['Avg Electricity Consumption per Household (kWh)'])<0.2].index\n",
    "# filter_corr=df_corr[abs(df_corr)>=0.1]\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(df_corr, annot=True,vmin=-1, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3LkCqWFXra9X",
    "outputId": "9739d233-c8e5-436c-a1f9-c150d84f3c9c"
   },
   "outputs": [],
   "source": [
    "# Heatmap on Avg Electricity Consumption per Household (kWh)\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "df_corr = df_numeric.corr()\n",
    "# df_corr_find = df_corr.copy()\n",
    "# low_cols = df_corr_find[abs(df_corr_find['Avg Electricity Consumption per Household (kWh)'])<0.2].index\n",
    "df_corr = df_corr.loc[['Avg Electricity Consumption per Household (kWh)']]\n",
    "#filter_corr=df_corr[abs(df_corr)>=0.1]\n",
    "plt.figure(figsize=(20, 1))\n",
    "sns.heatmap(df_corr, annot=True, vmin=-1, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap on Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fMXapeQfra9Y",
    "outputId": "6c7f3548-5e60-4b64-b10e-8113fb42386f"
   },
   "outputs": [],
   "source": [
    "# Heatmap on Avg Gas Supply per Household (ton)\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "df_corr = df_numeric.corr()\n",
    "df_corr = df_corr.loc[['Avg Gas Supply per Household (ton)']]\n",
    "plt.figure(figsize=(20, 1))\n",
    "sns.heatmap(df_corr, annot=True, vmin=-1, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap on Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BeyvcPYxra9Y",
    "outputId": "f97e1960-2981-456b-a4be-876b771414f6"
   },
   "outputs": [],
   "source": [
    "# Monthly average temperature, electricity consumption and gas supply\n",
    "triple_features_plot(df, 'Avg Temperature (Celsius)', 'Avg Electricity Consumption per Household (kWh)', 'Avg Gas Supply per Household (ton)', 'Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a1O8c_eRra9Y",
    "outputId": "01e6d5bb-ec49-4f22-c551-b52fd71d99ec"
   },
   "outputs": [],
   "source": [
    "# Monthly average electricity consumption box plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(data=df, x='Month', y='Avg Electricity Consumption per Household (kWh)')\n",
    "plt.title(\"Monthly Average Electricity Consumption (2016 ~ 2024)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Avg Electricity Consumption per Household (kWh)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XHFDOkxmra9Y",
    "outputId": "f227cb52-844e-45cb-bf22-1c16d5121e2c"
   },
   "outputs": [],
   "source": [
    "# Monthly average gas supply box plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(data=df, x='Month', y='Avg Gas Supply per Household (ton)')\n",
    "plt.title(\"Monthly Average Gas Supply (2016 ~ 2024)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Avg Gas Supply (ton)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6invchWLra9Y",
    "outputId": "450c6e6c-0f00-4dfc-8946-bcbc1bf1c992"
   },
   "outputs": [],
   "source": [
    "# Monthly temperature and precipitation plot\n",
    "dual_features_plot(a='Avg Temperature (Celsius)', b='Monthly Precipitation (mm)', c='Month')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LKnIjEa_ra9Y",
    "outputId": "20afdc10-1035-4e58-ccda-ec14decad330"
   },
   "outputs": [],
   "source": [
    "# Monthly small pan evapouration and avg relative humidity\n",
    "dual_features_plot(a='Small Pan Evaporation (mm)',b='Avg Relative Humidity (%)',c='Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oQNuDznhra9Y",
    "outputId": "272ae646-770d-412d-b407-804e3e042652"
   },
   "outputs": [],
   "source": [
    "# Monthly heat wave days & cold wave days & energy consumption\n",
    "bar_widths = 0.5\n",
    "color_a = 'tab:red'\n",
    "color_b = '#1e1690'\n",
    "color_c = 'tab:orange'\n",
    "col_a = 'Heat Wave Days'\n",
    "col_b = 'Cold Wave Days'\n",
    "col_c = 'Tropical Nights'\n",
    "data = df.groupby('Month')[['Number of Heat Wave Days', 'Number of Cold Wave Days','Number of Tropical Nights']].mean().reset_index()\n",
    "data_mean = df.groupby('Month')[['Avg Electricity Consumption per Household (kWh)']].mean().reset_index()\n",
    "fig, ax_1 = plt.subplots(figsize=(12, 8))\n",
    "months = data['Month'].unique()\n",
    "x = np.arange(len(months))\n",
    "rects1 = ax_1.bar(data['Month'], data['Number of Heat Wave Days'],label=col_a , color=color_a)\n",
    "rects2 = ax_1.bar(data['Month'],  data['Number of Cold Wave Days'],label=col_b, color=color_b)\n",
    "ax_1.set_xticks(data['Month'])\n",
    "ax_1.set_xlabel('Month')\n",
    "ax_1.set_ylabel('Days')\n",
    "ax_1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "#######\n",
    "col_d = 'Electricity Consumption(kWh)'\n",
    "ax_2 = ax_1.twinx()\n",
    "ax_2.set_ylabel('Avg Electricity Consumption per Household (kWh)')\n",
    "plot_c=ax_2.plot(data_mean['Month'], data_mean['Avg Electricity Consumption per Household (kWh)'], marker='o', color='grey',label=col_d )\n",
    "ax_2.tick_params(axis='y', labelcolor='blue')\n",
    "for x_val,y_val in zip(data_mean['Month'],data_mean['Avg Electricity Consumption per Household (kWh)']):\n",
    "\tax_2.text(x_val, y_val +(ax_2.get_ylim()[1] - ax_2.get_ylim()[0]) * 0.02,f'{y_val:.2f}',ha='center',va='bottom',fontsize=10,color='black')\n",
    "plt.title(f'Number of Heat Wave Days & Number of Cold Wave Days & Avg Electricity Consumption per Household (kWh) from 2016 to 2024')\n",
    "plt.grid(True, linestyle='--', alpha=0.6, zorder=0)\n",
    "plt.tight_layout()\n",
    "handles_to_combine = [rects1, rects2] + plot_c\n",
    "labels_to_combine = [h.get_label() for h in handles_to_combine]\n",
    "h1,l1 =ax_1.get_legend_handles_labels()\n",
    "h2,l2 =ax_2.get_legend_handles_labels()\n",
    "handles = h1 + h2\n",
    "labels = l1 + l2\n",
    "plt.legend(handles,labels,loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XdXSszekra9Y",
    "outputId": "e120f438-ee0c-4f00-b615-26b82e8ba09c"
   },
   "outputs": [],
   "source": [
    "# Monthly avg cloud cover and sunshine rate\n",
    "dual_features_plot(a='Avg Cloud Cover (1/10)', b='Sunshine Rate (%)', c='Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DPoWAP_xra9Y",
    "outputId": "c3c9f873-e42a-4ddd-cfc2-9aecac806036"
   },
   "outputs": [],
   "source": [
    "#Monthly sunlight and evaporation\n",
    "dual_features_plot(a='Small Pan Evaporation (mm)', b='Sunshine Rate (%)', c='Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qjWdiXFMra9Z",
    "outputId": "51108ac4-85cc-4a0b-bde0-b7f9ec31c4eb"
   },
   "outputs": [],
   "source": [
    "#Monthly avg wind speed and small pan evapouration\n",
    "dual_features_plot(a='Small Pan Evaporation (mm)', b='Avg Wind Speed (m/s)', c='Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RI7ZdVrCra9Z",
    "outputId": "1df8aafd-04a6-4b33-c61d-101c6c130862"
   },
   "outputs": [],
   "source": [
    "# Monthly ground temp plot\n",
    "df_mean=df.groupby('Month')['Avg Ground Temp (Celsius)'].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "df_mean.plot(kind='line',marker='o')\n",
    "plt.xticks(df_mean.index)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Avg Ground Temp')\n",
    "plt.title('Monthly ground temp plot')\n",
    "for x_val, y_val in df_mean.items():\n",
    "\tplt.text(x_val+0.1,y_val+0.55,f'{y_val:.1f}',ha='center',va='bottom',fontsize=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vCn_Ax4Ara9Z",
    "outputId": "1bb7f891-41c3-4865-e28a-547ea4baae65"
   },
   "outputs": [],
   "source": [
    "# Number of heat wave days per month by region\n",
    "one_feature_two_group('Region','Month','Number of Heat Wave Days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "L4DnPR24ra9Z",
    "outputId": "1fa601e5-7ec1-4cfa-dd70-097b0ba3904d"
   },
   "outputs": [],
   "source": [
    "# Avg electricity consumption per household (kWh) by region\n",
    "one_feature_two_group('Region','Year','Avg Electricity Consumption per Household (kWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SC0SuzInra9Z",
    "outputId": "c23245ed-b4f2-4b72-c136-d119c4fa9f56"
   },
   "outputs": [],
   "source": [
    "# Total electricity consumption (kWh) by region\n",
    "df['Total Electricity Consumption (kWh)'] = df['Number of Households'] * df['Avg Electricity Consumption per Household (kWh)']\n",
    "one_feature_two_group('Region','Year','Total Electricity Consumption (kWh)')\n",
    "df = df.drop(columns=['Total Electricity Consumption (kWh)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cQeeQJNvra9Z",
    "outputId": "29f96d83-beac-4dba-cf9c-7f468991db82"
   },
   "outputs": [],
   "source": [
    "# Avg Gas Supply per Household (ton) by region\n",
    "one_feature_two_group('Region','Year','Avg Gas Supply per Household (ton)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "28rCZHLGra9Z",
    "outputId": "1d981756-c436-4798-8ef0-b3c9beac99fe"
   },
   "outputs": [],
   "source": [
    "# Total gas supply (ton) by region\n",
    "df['Total Gas Supply (ton)'] = df['Number of Households'] * df['Avg Gas Supply per Household (ton)']\n",
    "one_feature_two_group('Region','Year','Total Gas Supply (ton)')\n",
    "df = df.drop(columns=['Total Gas Supply (ton)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2P6zwEMbra9Z",
    "outputId": "db76143f-d3ef-4cbe-bee0-264c3ec3dfc4"
   },
   "outputs": [],
   "source": [
    "# Precipitation by region\n",
    "one_feature_two_group('Region','Year','Monthly Precipitation (mm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV5ARUMSra9Z"
   },
   "source": [
    "### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M4-q_Tkra9Z"
   },
   "source": [
    "- Preprocessing steps safe to do before splitting\n",
    "<br>\n",
    "\n",
    "\t- Remove obvious duplicates\n",
    "\t- Fix incorrect labels\n",
    "\t- Drop irrelevant columns\n",
    "\t- Basic data cleaning such as fixing typos or unifying units\n",
    "\t- Handling missing data if not based on global statistics such as dropping rows w/ missing target\n",
    "\t- Changing boolean type to 0 / 1\n",
    "</br>\n",
    "\n",
    "- Preprocessing steps that should happen after train-test split\n",
    "<br>\n",
    "\t- Feature scaling / Normalisation\n",
    "\t- Encoding categorical variables\n",
    "\t- Dimensionality reduction\n",
    "\t- Outlier detection (if model is sensitive)\n",
    "\t- Feature selection based on correlation or variance\n",
    "\t- Creating time-based features if it involves future data\n",
    "\t- Data augmentation\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1xN2Hyg_ra9Z"
   },
   "outputs": [],
   "source": [
    "def stepwise_selection(X, y,\n",
    "\t\t\t\t\t   initial_list=[],\n",
    "\t\t\t\t\t   threshold_in=0.05,\n",
    "\t\t\t\t\t   threshold_out=0.10,\n",
    "\t\t\t\t\t   verbose=True):\n",
    "\t\"\"\"\n",
    "\tPerform a stepwise feature selection based on p-values from statsmodels OLS.\n",
    "\n",
    "\tParameters:\n",
    "\t- X : pd.DataFrame\n",
    "\t\tCandidate feature set (independent variables).\n",
    "\t- y : pd.Series or np.array\n",
    "\t\tTarget variable (dependent variable).\n",
    "\t- initial_list : list\n",
    "\t\tInitial list of features to start the selection process.\n",
    "\t- threshold_in : float\n",
    "\t\tp-value threshold for adding a feature (smaller = more strict).\n",
    "\t- threshold_out : float\n",
    "\t\tp-value threshold for removing a feature (larger = more lenient).\n",
    "\t- verbose : bool\n",
    "\t\tWhether to print progress during feature selection.\n",
    "\n",
    "\tReturns:\n",
    "\t- included : list\n",
    "\t\tThe final list of selected features.\n",
    "\t\"\"\"\n",
    "\n",
    "\tincluded = list(initial_list)  # Start with an initial list (could be empty)\n",
    "\n",
    "\twhile True:\n",
    "\t\tchanged = False  # Flag to track whether any feature was added or removed in the current iteration; If not, the loop will break\n",
    "\n",
    "\t\t# --- Forward Step ---\n",
    "\t\t# Try adding each feature not yet included and check p-values\n",
    "\t\texcluded = list(set(X.columns) - set(included))  # Compute the list of features not yet included in the model; These are the candidates for addition\n",
    "\t\tnew_pval = pd.Series(index=excluded, dtype=float)  # Initialise a Series to store the p-values of each excluded feature if it were to be added to the model\n",
    "\t\tfor new_column in excluded: # Iterate over all excluded features to assess their contribution\n",
    "\t\t\t# Fit OLS model with the current included features + this new one\n",
    "\t\t\tX_with_const = sm.add_constant(X[included + [new_column]]) # Prepare the design matrix with a constant term (intercept) and the current included features plus the candidate new feature\n",
    "\t\t\tmodel = sm.OLS(y, X_with_const).fit() # Fits an Ordinary Least Squares (OLS) linear regression model to the current design matrix\n",
    "\t\t\tnew_pval[new_column] = model.pvalues[new_column]  # Extract the p-value of the newly added feature and stores it\n",
    "\n",
    "\t\t# Add the feature with the lowest p-value if it's below threshold_in\n",
    "\t\tif not new_pval.empty and new_pval.min() < threshold_in: # Check whether the smallest p-value among the excluded features is statistically significant, i.e., below the inclusion threshold\n",
    "\t\t\tbest_pval = new_pval.idxmin()  # Feature with the smallest p-value\n",
    "\t\t\tincluded.append(best_pval)\n",
    "\t\t\tchanged = True\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f'Add {best_pval} with p-value {new_pval.min():.6f}')\n",
    "\n",
    "\t\t# --- Backward Step ---\n",
    "\t\t# Now check if any included feature should be removed\n",
    "\t\tX_with_const = sm.add_constant(X[included])\n",
    "\t\tmodel = sm.OLS(y, X_with_const).fit() # Re-fit the model using the current set of included features to re-calculate all p-values\n",
    "\t\tpvalues = model.pvalues.iloc[1:]  # Get p-values for all features excluding the intercept (which is the first value); These are the features being evaluated for possible removal\n",
    "\n",
    "\t\t# If any included feature has a p-value above threshold_out, remove the worst one\n",
    "\t\tif not pvalues.empty and pvalues.max() > threshold_out: # If the worst (largest) p-value among included features exceeds the exclusion threshold, it’s a candidate for removal\n",
    "\t\t\tworst_pval = pvalues.idxmax() # Find the feature with the worst (largest) p-value\n",
    "\t\t\tincluded.remove(worst_pval) # Remove this feature from the model\n",
    "\t\t\tchanged = True\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f'Drop {worst_pval} with p-value {pvalues.max():.6f}')\n",
    "\n",
    "\t\t# If no feature was added or removed, the process is done\n",
    "\t\tif not changed:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Print final selected features and summary\n",
    "\tif verbose:\n",
    "\t\tprint(\"\\nFinal Selected Variables:\")\n",
    "\t\tprint(included)\n",
    "\t\tfinal_X_with_const = sm.add_constant(X[included])\n",
    "\t\tfinal_model = sm.OLS(y, final_X_with_const).fit()\n",
    "\t\tprint(\"\\nFinal Model Summary:\")\n",
    "\t\tprint(final_model.summary())\n",
    "\n",
    "\treturn included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8HUQaFUBra9a"
   },
   "outputs": [],
   "source": [
    "def weak_corr_col(X: pd.DataFrame, Y: pd.Series, feature: str, threshold: float):\n",
    "\t\"\"\"\n",
    "\tIdentify numeric columns that are weakly correlated with a specified feature.\n",
    "\n",
    "\tParameters:\n",
    "\t- X (pd.DataFrame): Feature dataset.\n",
    "\t- Y (pd.Series): Target variable to be appended for correlation analysis.\n",
    "\t- feature (str): The column name to compare correlations against.\n",
    "\t- threshold (float): Absolute correlation threshold; columns with correlation\n",
    "\t\t\t\t\t\t less than this value will be returned.\n",
    "\n",
    "\tReturns:\n",
    "\t- pd.Index: Column names with absolute correlation to the specified feature\n",
    "\t\t\t\tless than the given threshold.\n",
    "\t\"\"\"\n",
    "\tdf = pd.concat([X, Y], axis=1)\n",
    "\tdf_numeric = df.select_dtypes(include=['number'])\n",
    "\tdf_corr = df_numeric.corr()\n",
    "\tweak_corr_cols = df_corr[abs(df_corr[feature]) < threshold].index\n",
    "\n",
    "\treturn weak_corr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cnQwjx1Qra9a"
   },
   "outputs": [],
   "source": [
    "# Data split for features and target\n",
    "X = df.drop(columns=['Avg Electricity Consumption per Household (kWh)', 'Avg Gas Supply per Household (ton)'])\n",
    "y = df['Avg Electricity Consumption per Household (kWh)']\n",
    "# y = df['Avg Gas Supply per Household (ton)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "67zAz1Kfra9a"
   },
   "outputs": [],
   "source": [
    "# Grab column names of each data type\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "datetime_cols = X.select_dtypes(include=['datetime']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "O7VsCEUxra9a"
   },
   "outputs": [],
   "source": [
    "# Data split for train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbExiv5Ira9a"
   },
   "outputs": [],
   "source": [
    "# X4 - Stepwise selection (Run before the region one hot encoding)\n",
    "stepwise_cols = stepwise_selection(X_train.select_dtypes(include=['number']), y_train, threshold_in=0.01, threshold_out=0.01, verbose=False)\n",
    "X4_train = X_train[stepwise_cols]\n",
    "X4_test = X_test[stepwise_cols]\n",
    "\n",
    "# Scaling for X4\n",
    "X4_scaler = StandardScaler()\n",
    "X4_train.loc[:, stepwise_cols] = X4_scaler.fit_transform(X4_train[stepwise_cols])\n",
    "X4_test.loc[:, stepwise_cols] = X4_scaler.transform(X4_test[stepwise_cols])\n",
    "\n",
    "# Region one hot encoding\n",
    "X4_train = pd.concat([X4_train, X_train['Region']], axis=1)\n",
    "X4_test = pd.concat([X4_test, X_test['Region']], axis=1)\n",
    "X4_train = pd.get_dummies(data=X4_train, columns=['Region'], dtype=int)\n",
    "X4_test = pd.get_dummies(data=X4_test, columns=['Region'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ErlZYpira9a"
   },
   "outputs": [],
   "source": [
    "# Region one hot encoding\n",
    "X_train = pd.get_dummies(data=X_train, columns=['Region'], dtype=int)\n",
    "X_test = pd.get_dummies(data=X_test, columns=['Region'], dtype=int)\n",
    "\n",
    "# Scaling for X\n",
    "X_scaler = StandardScaler()\n",
    "X_train[numerical_cols] = X_scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = X_scaler.transform(X_test[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Jrp3aOYXra9a"
   },
   "outputs": [],
   "source": [
    "# X0 - All\n",
    "X0_train = X_train.copy()\n",
    "X0_test = X_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eNFPh-jMra9a"
   },
   "outputs": [],
   "source": [
    "# X1 - Correlation\n",
    "weak_corr_cols = weak_corr_col(X_train, y_train,'Avg Electricity Consumption per Household (kWh)', 0.2)\n",
    "X1_train = X_train.drop(columns=(weak_corr_cols))\n",
    "X1_test = X_test.drop(columns=(weak_corr_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "m59Ebj5Bra9a"
   },
   "outputs": [],
   "source": [
    "# X2 - PCA\n",
    "pca = PCA()\n",
    "pca.fit_transform(X_train)\n",
    "\n",
    "# Find the features that contributes the most to PC 1\n",
    "weights_pca = abs(pca.components_)\n",
    "top_pc1_idx = np.argsort(weights_pca[0])[::-1][:15]\n",
    "top_pc1_name = [X_train.columns[i] for i in top_pc1_idx]\n",
    "\n",
    "# Grab features that contribute to PC 1\n",
    "top_features = list(set(top_pc1_name))\n",
    "pca_corr = X_train[top_features].corrwith(y_train)\n",
    "pca_corr\n",
    "\n",
    "# Drop columns w/ low contributions to PC 1\n",
    "X2_train = X_train[top_features]\n",
    "X2_test = X_test[top_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "5QDjGn0Jra9e"
   },
   "outputs": [],
   "source": [
    "# X3 - VIF\n",
    "df_vif = pd.DataFrame()\n",
    "df_vif['feature'] = X_train.columns\n",
    "df_vif['VIF'] = [variance_inflation_factor(X_train, i) for i in range(X_train.shape[1])]\n",
    "df_vif\n",
    "\n",
    "vif_cols = df_vif[df_vif['VIF'] > 100]['feature']\n",
    "X3_train = X_train.drop(columns=vif_cols)\n",
    "X3_test = X_test.drop(columns=vif_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m4Rs7WGra9e"
   },
   "source": [
    "### **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "CjoDtLozra9e"
   },
   "outputs": [],
   "source": [
    "def adj_r2_score(y_true: pd.Series, y_pred: pd.Series, X_features: pd.DataFrame) -> float:\n",
    "\t\"\"\"\n",
    "\tCompute the adjusted R² (coefficient of determination).\n",
    "\n",
    "\tAdjusted R² adjusts the regular R² score for the number of predictors (features)\n",
    "\tin the model. It penalizes the R² score for adding unnecessary predictors, helping\n",
    "\tto avoid overfitting.\n",
    "\n",
    "\tParameters:\n",
    "\t- y_true : array-like of shape (n_samples,)\n",
    "\t\tTrue target values.\n",
    "\t- y_pred : array-like of shape (n_samples,)\n",
    "\t\tPredicted target values from the model.\n",
    "\t- X_features : array-like of shape (n_samples, n_features)\n",
    "\t\tData used for getting the number of predictors (features) used in the model.\n",
    "\n",
    "\tReturns:\n",
    "\t- adj_r2 : float\n",
    "\t\tThe adjusted R-squared score. Returns NaN if the number of predictors is too large\n",
    "\t\tfor the formula to be valid (i.e., n <= p + 1).\n",
    "\t\"\"\"\n",
    "\tn = len(y_true)\n",
    "\tp = X_features.shape[1]\n",
    "\n",
    "\t# Prevent division by zero or negative denominator\n",
    "\tif n <= p + 1:\n",
    "\t\twarnings.warn(\"Adjust R^2 is undefined.\")\n",
    "\t\treturn float('nan')\n",
    "\n",
    "\treturn 1 - (1 - r2_score(y_true, y_pred)) * (n - 1) / (n - p - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bcYtHNV4ra9e"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model: BaseEstimator,\n",
    "\t\t\t\t   X_test: pd.DataFrame,\n",
    "\t\t\t\t   y_test: pd.Series) -> Tuple[pd.Series, float, float, float, float]:\n",
    "\t\"\"\"\n",
    "\tTrain a regression model and evaluates its performance on the test set.\n",
    "\n",
    "\tParameters:\n",
    "\t\tmodel (BaseEstimator): The regression model to train (already instantiated, e.g., from GridSearchCV.best_estimator_).\n",
    "\t\tX_test (pd.DataFrame): Testing features.\n",
    "\t\ty_test (pd.Series): Testing target values.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple containing:\n",
    "\t\t\t- y_pred (pd.Series): Predicted target values on the test set.\n",
    "\t\t\t- mae (float): Mean Absolute Error.\n",
    "\t\t\t- rmse (float): Root Mean Squared Error.\n",
    "\t\t\t- r2 (float): R-squared score.\n",
    "\t\t\t- adj_r2 (float): Adjusted R-squared score.\n",
    "\t\"\"\"\n",
    "\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\ty_pred = model.predict(X_test)\n",
    "\tmae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
    "\tmse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "\trmse = math.sqrt(mse)\n",
    "\tr2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "\tadj_r2 = adj_r2_score(y_true=y_test, y_pred=y_pred, X_features=X_test)\n",
    "\n",
    "\treturn y_pred, mae, rmse, r2, adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "QWnVC7Pwra9e"
   },
   "outputs": [],
   "source": [
    "def generate_combinations(models_dict: Dict[str, Any], max_k:int, min_k: int) -> List[Tuple[Tuple[str, Any], ...]]:\n",
    "\t\"\"\"\n",
    "    Generate all possible combinations of base learners from a dictionary of models,\n",
    "    with combination sizes ranging from min_k to max_k.\n",
    "\n",
    "    Parameters:\n",
    "    models_dict : dict\n",
    "        A dictionary where keys are model names and values are trained model objects.\n",
    "        Example: {'ElasticNet': model_1, 'XGBRegressor': model_2}\n",
    "\n",
    "    min_k : int, optional (default=2)\n",
    "        The minimum number of base models in a combination.\n",
    "\n",
    "    max_k : int, optional (default=6)\n",
    "        The maximum number of base models in a combination.\n",
    "\n",
    "    Returns:\n",
    "    all_combos : list of tuples\n",
    "        Each tuple contains a combination of (model_name, model_object) pairs.\n",
    "        Example: [(('ElasticNet', model1), ('XGBRegressor', model2)), ...]\n",
    "    \"\"\"\n",
    "\t# Convert the dictionary into a list of (key, value) tuples\n",
    "\titems = list(models_dict.items())\n",
    "\n",
    "\t# Initialisation\n",
    "\tcombinations = []\n",
    "\n",
    "\t# Ensure max_k does not exceed the number of available models\n",
    "\tmax_k = min(max_k, len(items))\n",
    "\n",
    "\t# Generate combinations of size k from min_k to max_k\n",
    "\tfor k in range(min_k, max_k + 1):\n",
    "\t\tcombos = itertools.combinations(items, k)\n",
    "\t\tfor combo in combos:\n",
    "\t\t\tcombinations.append(combo)\n",
    "\n",
    "\treturn combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO13SMTlra9e"
   },
   "outputs": [],
   "source": [
    "def plot_model_prediction (model: pd.DataFrame, feature_list: List):\n",
    "\t\"\"\"\n",
    "\tVisualise predicted vs actual values for different feature sets using scatter plots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pd.DataFrame\n",
    "        A pandas DataFrame containing prediction results.\n",
    "        Required columns in the DataFrame:\n",
    "            - 'Data': str or int, identifier for each feature set (should match elements in global variable X)\n",
    "            - 'Model': str, name of the model\n",
    "            - 'y_test': array-like, actual target values\n",
    "            - 'y_pred': array-like, predicted target values\n",
    "\tfeature_list : list\n",
    "\t\tList of features (X0, X1, etc.)\n",
    "\t\"\"\"\n",
    "\n",
    "\tnum_plots = len(feature_list)\n",
    "\tnum_cols = 3\n",
    "\tnum_rows = math.ceil(num_plots / num_cols)\n",
    "\tfig, axes = plt.subplots(num_rows, num_cols, figsize=(13,13))\n",
    "\taxes =  axes.flatten()\n",
    "\n",
    "\tfor i, feature in enumerate(feature_list):\n",
    "\t\tdf = model[model['Data']==feature]\n",
    "\t\tax = axes[i]\n",
    "\t\tlabel = df.iloc[0]['Model']\n",
    "\t\tx_data = np.array(df.iloc[0]['y_test'])\n",
    "\t\ty_data = np.array(df.iloc[0]['y_pred'])\n",
    "\t\tax.scatter(x=x_data, y=y_data, label=label)\n",
    "\n",
    "\t\tmin_val = min(ax.get_xlim()[0], ax.get_ylim()[0], x_data.min(), y_data.min())\n",
    "\t\tmax_val = max(ax.get_xlim()[1], ax.get_ylim()[1], x_data.max(), y_data.max())\n",
    "\t\tpadding = (max_val - min_val) * 0.05\n",
    "\t\tmin_val -= padding\n",
    "\t\tmax_val += padding\n",
    "\n",
    "\t\tline_points = np.array([min_val, max_val])\n",
    "\t\tax.plot(line_points, line_points, color='red', linestyle='--', linewidth=1, label='y=x')\n",
    "\t\tax.set_xlim(min_val, max_val)\n",
    "\t\tax.set_ylim(min_val, max_val)\n",
    "\t\tax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "\t\tax.set_ylabel('Predicted y')\n",
    "\t\tax.set_xlabel('Actual y')\n",
    "\t\tax.set_title(f'{feature} Feature')\n",
    "\t\tax.grid(True)\n",
    "\t\tax.legend()\n",
    "\n",
    "\tfor j in range(i + 1, len(axes)):\n",
    "\t\taxes[j].set_visible(False)\n",
    "\n",
    "\tfig.suptitle(model.iloc[0]['Model'], fontsize=30, fontweight='bold', y=0.9)\n",
    "\tplt.tight_layout(w_pad=0,h_pad=-10)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OAkCGccQra9f"
   },
   "outputs": [],
   "source": [
    "# Config setting\n",
    "# Linear regression\n",
    "model_linear_regression = LinearRegression()\n",
    "hyperparams_linear_regression = {'fit_intercept': [True]}\n",
    "config_linear_regression = [model_linear_regression, hyperparams_linear_regression]\n",
    "\n",
    "# Elastic net\n",
    "model_en = ElasticNet(random_state=42)\n",
    "hyperparams_en = {'alpha': [0.1, 1, 10],\n",
    "\t\t\t\t  'l1_ratio': [0.1, 0.5, 0.9]}\n",
    "config_en = [model_en, hyperparams_en]\n",
    "\n",
    "# SVR\n",
    "model_svr = SVR()\n",
    "hyperparams_svr = {'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "\t\t\t\t   'C':[10, 100, 1000],\n",
    "\t\t\t\t   'epsilon':[0.3,0.5,0.8]}\n",
    "config_svr = [model_svr, hyperparams_svr]\n",
    "\n",
    "# Decision tree\n",
    "model_dt = DecisionTreeRegressor(random_state=42)\n",
    "hyperparams_dt = {'criterion': ['absolute_error', 'squared_error'],\n",
    "\t\t\t\t  'max_depth': [3, 5, 7, 10, None],\n",
    "\t\t\t\t  'min_samples_split': [2, 5, 10],\n",
    "\t\t\t\t  'min_samples_leaf': [1, 2, 4],\n",
    "\t\t\t\t  'max_features': ['sqrt', 'log2', None]}\n",
    "config_dt = [model_dt, hyperparams_dt]\n",
    "\n",
    "# Random forest\n",
    "model_rf = RandomForestRegressor(random_state=42)\n",
    "hyperparams_rf = {'n_estimators': [10, 50, 100],\n",
    "\t\t\t\t  'max_depth': [None, 10, 20],\n",
    "\t\t\t\t  'min_samples_split': [2, 5, 10],\n",
    "\t\t\t\t  'min_samples_leaf': [1, 2, 4],\n",
    "\t\t\t\t  'bootstrap': [True, False],}\n",
    "config_rf = [model_rf, hyperparams_rf]\n",
    "\n",
    "# AdaBoost\n",
    "model_ada = AdaBoostRegressor(random_state=42)\n",
    "hyperparams_ada = {'n_estimators': [30, 50, 100],\n",
    "\t\t\t\t   'learning_rate': [0.01, 0.005, 0.001],}\n",
    "config_ada = [model_ada, hyperparams_ada]\n",
    "\n",
    "# XGB\n",
    "model_xgb = XGBRegressor(random_state=42)\n",
    "hyperparams_xgb = {'n_estimators': [100, 200, 300],\n",
    "\t\t\t\t   'learning_rate': [0.01, 0.05, 0.1],\n",
    "\t\t\t\t   'max_depth': [3, 5, 7]}\n",
    "config_xgb = [model_xgb, hyperparams_xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otOvXTH6ra9f"
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "models = [config_linear_regression, config_en, config_svr, config_dt, config_rf, config_ada, config_xgb]\n",
    "X_trains = [X0_train, X1_train, X2_train, X3_train, X4_train]\n",
    "X_tests = [X0_test, X1_test, X2_test, X3_test, X4_test]\n",
    "\n",
    "# models = [config_linear_regression, config_en, config_svr]\n",
    "# X_trains = [X0_train, X1_train]\n",
    "# X_tests = [X0_test, X1_test]\n",
    "\n",
    "# Initialisation\n",
    "best_estimators_dict = {}\n",
    "best_params_dict = {}\n",
    "regression_results = []\n",
    "stacking_results = []\n",
    "feature_importances = []\n",
    "\n",
    "# Iterate over each feature (X0, X1, ...)\n",
    "for data_idx, (X_train, X_test) in enumerate(zip(X_trains, X_tests)):\n",
    "\tdata_tag = f'X{data_idx}' # Label for current dataset\n",
    "\t#print(f\"Running on {data_tag}\")\n",
    "\n",
    "\tbest_estimators_dict[data_tag] = {}\n",
    "\tbest_params_dict[data_tag] = {}\n",
    "\n",
    "\t# Iterate over all models\n",
    "\tfor model_idx, (estimator, param_grid) in enumerate(models):\n",
    "\t\tmodel_name = estimator.__class__.__name__\n",
    "\n",
    "\t\t# Training\n",
    "\t\tgrid_search = GridSearchCV(estimator=estimator,\n",
    "\t\t\t\t    \t\t\t   param_grid=param_grid,\n",
    "\t\t\t\t\t\t\t\t   cv=5)\n",
    "\t\tstart_time = time.time()\n",
    "\t\tgrid_search.fit(X_train, y_train)\n",
    "\t\tend_time = time.time()\n",
    "\t\ttraining_time = end_time - start_time\n",
    "\n",
    "\t\t# Store the best model and its hyperparameters\n",
    "\t\tbest_model = grid_search.best_estimator_\n",
    "\t\tbest_params = grid_search.best_params_\n",
    "\t\tbest_estimators_dict[data_tag][model_name] = best_model\n",
    "\t\tbest_params_dict[data_tag][model_name] = best_params\n",
    "\n",
    "\t\t# Testing and evaluating\n",
    "\t\ty_pred, mae, rmse, r2, adj_r2 = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "\t\t# Log result\n",
    "\t\tregression_results.append({'y_test': list(y_test),\n",
    "\t\t\t\t\t\t\t\t   'y_pred': list(y_pred),\n",
    "\t\t\t\t\t\t\t\t   'Data': data_tag,\n",
    "\t\t\t\t\t\t\t\t   'Model': model_name,\n",
    "\t\t\t\t\t\t\t\t   'Training time': training_time,\n",
    "\t\t\t\t\t\t\t\t   'Best hyperparams': best_params,\n",
    "\t\t\t\t\t\t\t\t   'MAE': mae,\n",
    "\t\t\t\t\t\t\t\t   'RMSE': rmse,\n",
    "\t\t\t\t\t\t\t\t   'R^2': r2,\n",
    "\t\t\t\t\t\t\t\t   'Adjusted R^2': adj_r2})\n",
    "\n",
    "\t# Stacking\n",
    "\testimators = best_estimators_dict[data_tag]\n",
    "\n",
    "\t# Rank models by adjusted R^2\n",
    "\tdf_results = (pd.DataFrame([result for result in regression_results if result['Data']==data_tag])\n",
    "\t\t\t\t  .sort_values(by='Adjusted R^2', ascending=False)\n",
    "\t\t\t\t  .reset_index(drop=True))\n",
    "\n",
    "\t# Select the meta learner w/ top performances\n",
    "\tmeta_learner_name = df_results.iloc[0]['Model']\n",
    "\tmeta_model = estimators[meta_learner_name]\n",
    "\n",
    "\t# Select next best models as base learners\n",
    "\tbase_candidates = df_results.iloc[1:]['Model'].tolist()\n",
    "\tbase_objects = {name: estimators[name] for name in base_candidates}\n",
    "\n",
    "\tbest_adj_r2 = 0\n",
    "\tbest_combo = None\n",
    "\tbest_stacking_model = None\n",
    "\n",
    "\t# print(f\"[{data_tag}] Meta Learner: {meta_learner_name}\")\n",
    "\t# print(f\"[{data_tag}] Base candidates: {list(base_objects.keys())}\")\n",
    "\n",
    "\t# Try all combinations of base learners\n",
    "\tfor combo in generate_combinations(base_objects, max_k=6, min_k=1):\n",
    "\t\t# print(f\"[{data_tag}] Trying combo: {combo}\")\n",
    "\t\tcombo_names, estimators_combo = zip(*combo)\n",
    "\n",
    "\t\t# Build the stacking model\n",
    "\t\tstack_model = StackingRegressor(estimators=list(zip(combo_names, estimators_combo)),\n",
    "\t\t\t\t\t\t\t\t\t\tfinal_estimator=meta_model,\n",
    "\t\t\t\t\t\t\t\t\t\tcv=5,\n",
    "\t\t\t\t\t\t\t\t\t\tn_jobs=-1)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\t# Training and testing\n",
    "\t\t\tstack_model.fit(X_train, y_train)\n",
    "\t\t\ty_pred, mae, rmse, r2, adj_r2 = evaluate_model(stack_model, X_test, y_test)\n",
    "\n",
    "\t\t\t# Update adjusted R^2 score\n",
    "\t\t\tif adj_r2 > best_adj_r2:\n",
    "\t\t\t\tbest_adj_r2 = adj_r2\n",
    "\t\t\t\tbest_combo = combo_names\n",
    "\t\t\t\tbest_stacking_model = stack_model\n",
    "\t\t\t\t# print(f\">>> New best adjusted R^2: {best_adj_r2}\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error during stacking with combination {combo_names}: {e}\")\n",
    "\n",
    "\t# Log the best stacking model if one was found\n",
    "\tif best_stacking_model:\n",
    "\t\ty_pred, mae, rmse, r2, adj_r2 = evaluate_model(best_stacking_model, X_test, y_test)\n",
    "\n",
    "\t\tstacking_results.append({'y_test': list(y_test),\n",
    "\t\t\t\t\t\t   \t\t 'y_pred': list(y_pred),\n",
    "\t\t\t\t\t\t\t\t 'Data': data_tag,\n",
    "\t\t\t\t\t\t\t\t 'Model': 'Stacking',\n",
    "\t\t\t\t\t\t\t\t 'Training time': 'N/A',\n",
    "\t\t\t\t\t\t\t\t 'Best hyperparams': {'Meta': meta_learner_name, 'Base': list(best_combo)},\n",
    "\t\t\t\t\t\t\t\t 'MAE': mae,\n",
    "\t\t\t\t\t\t\t\t 'RMSE': rmse,\n",
    "\t\t\t\t\t\t\t\t 'R^2': r2,\n",
    "\t\t\t\t\t\t\t\t 'Adjusted R^2': adj_r2})\n",
    "\n",
    "\t\tif isinstance(X_test, pd.DataFrame):\n",
    "\t\t\tfeature_names = X_test.columns\n",
    "\t\telse:\n",
    "\t\t\tfeature_names = [f'Feature {i}' for i in range(X_test.shape[1])]\n",
    "\t\t\tX_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "\t\t\tX_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "\t\t# Compute permutation importance\n",
    "\t\tresult = permutation_importance(best_stacking_model,\n",
    "\t\t\t\t\t\t\t\t\t\tX_test,\n",
    "\t\t\t\t\t\t\t\t\t\ty_test,\n",
    "\t\t\t\t\t\t\t\t\t\tn_repeats=10,\n",
    "\t\t\t\t\t\t\t\t\t\trandom_state=42,\n",
    "\t\t\t\t\t\t\t\t\t\tn_jobs=-1,\n",
    "\t\t\t\t\t\t\t\t\t\tscoring='r2')\n",
    "\n",
    "\t\tfeature_names = X_test.columns if isinstance(X_test, pd.DataFrame) else [f'Feature {i}' for i in range(X_test.shape[1])]\n",
    "\n",
    "\t\tdf_perm = (pd.DataFrame({'Data': data_tag,\n",
    "\t\t\t\t\t\t\t\t 'Feature': feature_names,\n",
    "\t\t\t\t\t\t\t\t 'Mean importance': result.importances_mean,\n",
    "\t\t\t\t\t\t\t\t 'Std importance': result.importances_std})\n",
    "\t\t\t\t   .sort_values(by='Mean importance', ascending=False)\n",
    "\t\t\t\t   .reset_index(drop=True))\n",
    "\n",
    "\t\tfeature_importances.append(df_perm)\n",
    "\n",
    "\t\t# Compute drop-column importance\n",
    "\t\t# baseline_score = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "\t\t# importances = []\n",
    "\t\t# for col in feature_names:\n",
    "\t\t# \t# Drop the feature\n",
    "\t\t# \tX_train_drop = X_train.drop(columns=[col])\n",
    "\t\t# \tX_test_drop = X_test.drop(columns=[col])\n",
    "\n",
    "\t\t# \t# Re-train the model on reduced feature set\n",
    "\t\t# \tstack_model_temp = clone(best_stacking_model)\n",
    "\t\t# \ttry:\n",
    "\t\t# \t\tstack_model_temp.fit(X_train_drop, y_train)\n",
    "\t\t# \t\tdrop_score = r2_score(y_true=y_test, y_pred=stack_model_temp.predict(X_test_drop))\n",
    "\t\t# \t\timportance = baseline_score - drop_score\n",
    "\t\t# \texcept Exception as e:\n",
    "\t\t# \t\tprint(f\"Error computing drop-column importance for {col}: {e}\")\n",
    "\n",
    "\t\t# \timportances.append(importance)\n",
    "\n",
    "\t\t# df_dropcol = (pd.DataFrame({'Data': data_tag,\n",
    "\t\t# \t\t\t\t\t\t\t'Feature': feature_names,\n",
    "\t\t# \t\t\t\t\t\t\t'Drop-column importance': importances})\n",
    "\t\t# \t\t\t  .sort_values(by='Drop-column importance', ascending=False)\n",
    "\t\t# \t\t\t  .reset_index(drop=True))\n",
    "\n",
    "\t\t# feature_importances.append(df_dropcol)\n",
    "\n",
    "\t\t# Compute SHAP\n",
    "\t\t# X_sample = shap.utils.sample(X=X_train, nsamples=100, random_state=42) # Sample on a subset of data\n",
    "\n",
    "\t\t# try:\n",
    "\t\t# \t# Use KernelExplainer for model-agnostic SHAP\n",
    "\t\t# \texplainer = shap.Explainer(best_stacking_model.predict, X_sample)\n",
    "\t\t# \tshap_values = explainer.shap_values(X_sample, nsamples='auto')\n",
    "\n",
    "\t\t# \t# Compute mean absolute SHAP value per feature\n",
    "\t\t# \tmean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "\t\t# except Exception as e:\n",
    "\t\t# \tprint(f\"SHAP importance error for {data_tag}: {e}\")\n",
    "\n",
    "\t\t# df_shap = (pd.DataFrame({'Data': data_tag,\n",
    "\t\t# \t\t\t\t\t\t 'Feature': feature_names,\n",
    "\t\t# \t\t\t\t\t\t 'SHAP importance': mean_abs_shap})\n",
    "\t\t# \t\t   .sort_values(by='SHAP importance', ascending=False)\n",
    "\t\t# \t\t   .reset_index(drop=True))\n",
    "\n",
    "\t\t# feature_importances.append(df_shap)\n",
    "\n",
    "# Final DataFrame\n",
    "df_regression = pd.DataFrame(regression_results)\n",
    "df_stacking = pd.DataFrame(stacking_results)\n",
    "df_models = pd.concat([df_regression, df_stacking])\n",
    "df_feature_importance = pd.concat(feature_importances, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression.to_csv('data/result/result_regression.csv', index=False)\n",
    "df_stacking.to_csv('data/result/result_stacking.csv', index=False)\n",
    "df_feature_importance.to_csv('data/result/feature_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "hsiPLYTIra9f",
    "outputId": "bb755e11-6da4-4ded-dabb-ab4a77a11ff4"
   },
   "outputs": [],
   "source": [
    "# Total Result\n",
    "result_models = df_models.sort_values(by=['Model','Data'], ascending=[True, True])\n",
    "result_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Vf4FUORJra9f"
   },
   "outputs": [],
   "source": [
    "# Model list\n",
    "model_linear_regression = result_models[result_models['Model']=='LinearRegression']\n",
    "model_en = result_models[result_models['Model']=='ElasticNet']\n",
    "model_svr = result_models[result_models['Model']=='SVR']\n",
    "model_dt = result_models[result_models['Model']=='DecisionTreeRegressor']\n",
    "model_rf = result_models[result_models['Model']=='RandomForestRegressor']\n",
    "model_ada = result_models[result_models['Model']=='AdaBoostRegressor']\n",
    "model_xgb = result_models[result_models['Model']=='XGBRegressor']\n",
    "model_stacking = result_models[result_models['Model']=='Stacking']\n",
    "model_list = [model_linear_regression, model_en, model_svr, model_dt, model_rf, model_ada, model_xgb, model_stacking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ne_g4qOra9f",
    "outputId": "b80d0184-8d72-4ebf-914b-981c3301e132"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m feature_list = [\u001b[33m'\u001b[39m\u001b[33mX0\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mX1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mX2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mX3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mX4\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_list\u001b[49m:\n\u001b[32m      3\u001b[39m \tplot_model_prediction(model, feature_list=feature_list)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_list' is not defined"
     ]
    }
   ],
   "source": [
    "feature_list = ['X0', 'X1', 'X2', 'X3', 'X4']\n",
    "for model in model_list:\n",
    "\tplot_model_prediction(model, feature_list=feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deP2Yiarra9f"
   },
   "source": [
    "### **Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling for y\n",
    "y_scaler = StandardScaler()\n",
    "y_train_2d = y_train.values.reshape(-1, 1) # Change y into 2-D before scaling\n",
    "y_test_2d = y_test.values.reshape(-1, 1)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_2d)\n",
    "y_test_scaled = y_scaler.transform(y_test_2d)\n",
    "y_train_scaled = y_train_scaled.flatten() # Change y back to 1-D\n",
    "y_test_scaled = y_test_scaled.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfy2JdVWra9f",
    "outputId": "fc5d1f19-1c26-4ebd-b160-83e54b3bcdb2"
   },
   "outputs": [],
   "source": [
    "# Seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# NN hyperparameters\n",
    "hyperparams_nn = {'units': [64, 128, 256],\n",
    "\t\t\t\t  'activation': ['relu', 'tanh'],\n",
    "\t\t\t\t  'l2': [0.001, 0.0001],\n",
    "\t\t\t\t  'optimizer': ['adam'],\n",
    "\t\t\t\t  'epochs': [100, 150],\n",
    "\t\t\t\t  'batch_size': [16, 32]}\n",
    "\n",
    "# NN model\n",
    "def nn_model(input_dim, units, activation, l2, optimizer):\n",
    "\tmodel = keras.Sequential([keras.Input(shape=(input_dim,)),\n",
    "\t\t\t\t\t\t\t  keras.layers.Dense(units, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "\t\t\t\t\t\t\t  keras.layers.Dense(units//2, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "\t\t\t\t\t\t\t  keras.layers.Dense(units//4, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "\t\t\t\t\t\t\t  keras.layers.Dense(1)])\n",
    "\tmodel.compile(optimizer=optimizer, loss='mse')\n",
    "\treturn model\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "best_model = None\n",
    "nn_results = []\n",
    "\n",
    "# Iterate through different hyperparameters to find best hyperparameters\n",
    "for params in ParameterGrid(hyperparams_nn):\n",
    "\t#print(f\"Trying params: {params}\")\n",
    "\tmodel = nn_model(input_dim=X4_train.shape[1],\n",
    "\t\t\t\t\t units=params['units'],\n",
    "\t\t\t\t\t activation=params['activation'],\n",
    "\t\t\t\t\t l2=params['l2'],\n",
    "\t\t\t\t\t optimizer=params['optimizer'])\n",
    "\t# Training\n",
    "\thistory = model.fit(X4_train, y_train_scaled,\n",
    "\t\t\t\t\t\tepochs=params['epochs'],\n",
    "\t\t\t\t\t\tbatch_size=params['batch_size'],\n",
    "\t\t\t\t\t\tverbose=0, validation_split=0.2)\n",
    "\t\n",
    "\t# Testing\n",
    "\ty_pred = model.predict(X4_test)\n",
    "\ty_pred = y_scaler.inverse_transform(y_pred).flatten() #Inverse scaling\n",
    "\n",
    "\t# Evaluation\n",
    "\tadj_r2 = adj_r2_score(y_true=y_test, y_pred=y_pred, X_features=X4_test)\n",
    "\t\n",
    "\t# Update based on adjusted R^2\n",
    "\tif adj_r2 > best_score: \n",
    "\t\tbest_score = adj_r2\n",
    "\t\tbest_params = params\n",
    "\t\tbest_model = model\n",
    "\t\tr2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "\t\tmae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
    "\t\tmse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "\t\trmse = math.sqrt(mse)\n",
    "\t\t\n",
    "\t\t#print(f\"✅ New Best Adjusted R²: {best_score:.4f} with {best_params}\")\n",
    "\n",
    "# Result\t\t\n",
    "# print(f\"\\n🎯 Best Hyperparameters Found: {best_params}\")\n",
    "\n",
    "nn_results.append({'y_test': list(y_test),\n",
    "\t\t\t\t   'y_pred': list(y_pred),\n",
    "\t\t\t\t   'Data': 'X4',\n",
    "\t\t\t\t   'Model': 'NN',\n",
    "\t\t\t\t   'Training time': 'N/A',\n",
    "\t\t\t\t   'Best hyperparams': best_params,\n",
    "\t\t\t\t   'MAE': mae,\n",
    "\t\t\t\t   'RMSE': rmse,\n",
    "\t\t\t\t   'R^2': r2,\n",
    "\t\t\t\t   'Adjusted R^2': best_score})\n",
    "df_nn = pd.DataFrame(nn_results)\n",
    "df_models = pd.concat([df_models, df_nn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result file\n",
    "df_nn.to_csv('data/result/result_nn.csv', index=False)\n",
    "df_models.to_csv('data/result/model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK7kuk1era9g"
   },
   "source": [
    "### **Visualisation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72l_kEbSra9g"
   },
   "source": [
    "1. ~2023 Train 2024 Test\n",
    "2. ~2022 Train 2023 Test\n",
    "3. ~2021 Train 2022 Test\n",
    "4. ~2020 Train 2021 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1iHr59Tra9g"
   },
   "outputs": [],
   "source": [
    "result_model = pd.read_csv('data/cleaned/Result_Model.csv')\n",
    "result_stack = pd.read_csv('data/cleaned/Result_Stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0lkNrhIra9g"
   },
   "outputs": [],
   "source": [
    "#New Data predict\n",
    "#df split\n",
    "df = pd.read_csv('data/cleaned/energy_consumption.csv')\n",
    "df_new = df[df['Year']<=2024] #2023 2022 \n",
    "test_condition= (df_new['Year'] == 2024) #2023 2022 \n",
    "df_test=df_new[test_condition]\n",
    "df_train=df_new[~test_condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cViclS7Ara9g"
   },
   "outputs": [],
   "source": [
    "#Train data / Test Data\n",
    "#Train\n",
    "X=df_train.drop(columns=['Avg Electricity Consumption per Household (kWh)'])\n",
    "y=df_train['Avg Electricity Consumption per Household (kWh)']\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "#Test\n",
    "X_test = df_test.drop(columns=['Avg Electricity Consumption per Household (kWh)'])\n",
    "y_test = df_test['Avg Electricity Consumption per Household (kWh)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u5H5B5kra9g",
    "outputId": "2348ac4c-87dc-4e9e-aca6-af6fb8725559"
   },
   "outputs": [],
   "source": [
    "#Feature Selection->X4\n",
    "X_test=X_test[selected_variables]\n",
    "X_train=X_train[selected_variables]\n",
    "X_val=X_val[selected_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDm-0tdrra9g"
   },
   "outputs": [],
   "source": [
    "#Scaler\n",
    "scale=StandardScaler()\n",
    "numeric=X_train.select_dtypes(include=['number']).columns\n",
    "X_train[numeric]=scale.fit_transform(X_train[numeric])\n",
    "X_val[numeric]=scale.transform(X_val[numeric])\n",
    "X_test[numeric]=scale.transform(X_test[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2mqF11Vra9g"
   },
   "outputs": [],
   "source": [
    "#one-hot-encoding\n",
    "X_train=pd.get_dummies(data=X_train, columns=['Region'], dtype=int)\n",
    "X_val=pd.get_dummies(data=X_val, columns=['Region'], dtype=int)\n",
    "X_test=pd.get_dummies(data=X_test, columns=['Region'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGq-MjmWra9g",
    "outputId": "de9a1fb2-b700-4664-f49f-2a2c80962cfc"
   },
   "outputs": [],
   "source": [
    "#인스턴스 model(param)\n",
    "instantiated_models_info = []\n",
    "#시각화 이름\n",
    "visual_df_name=[]\n",
    "num=0\n",
    "#첫번째 y_pred에 있는 모델만 출력\n",
    "result_high_pred=result_model.iloc[0]['y_test'] #왜 같은값이 두개씩 출력되지?\n",
    "instant_condition=((result_model['Data_number']=='X4')&(result_model['y_test']==result_high_pred))\n",
    "model_get=result_model[instant_condition]\n",
    "model_get\n",
    "\n",
    "#반복을 통해서 추출\n",
    "for index,row in model_get.iterrows():\n",
    "    model_name_str=row['Model']\n",
    "    params_list_of_lists = row['best_params']\n",
    "    params_list_of_lists = eval(params_list_of_lists)\n",
    "\n",
    "    #모델명 클래스 찾기\n",
    "    model_class = None #초기화\n",
    "    model_maping={\n",
    "        'LinearRegression':LinearRegression,\n",
    "        'ElasticNet':ElasticNet,\n",
    "        'RandomForestRegressor':RandomForestRegressor,\n",
    "        'AdaBoostRegressor':AdaBoostRegressor,\n",
    "        'DecisionTreeRegressor':DecisionTreeRegressor,\n",
    "        'SVR':SVR,\n",
    "        'XGBRegressor':XGBRegressor\n",
    "    }\n",
    "    model_class = model_maping.get(model_name_str)\n",
    "\n",
    "    #하이퍼파라미터 리스트->딕션너리\n",
    "    hyperparams_dict = {} # 최종 딕셔너리 초기화\n",
    "    hyperparams_dict = dict(params_list_of_lists)\n",
    "\n",
    "\n",
    "\n",
    "    #모델 인스턴스 생성\n",
    "    model = model_class(**hyperparams_dict)\n",
    "\n",
    "\n",
    "    #모델 저장\n",
    "    instantiated_models_info.append({'name':model_name_str,'instance':model})\n",
    "    print(num)\n",
    "    #모델 훈련\n",
    "    start_time=time.time()\n",
    "    model.fit(X_train,y_train)\n",
    "    end_time=time.time()\n",
    "\n",
    "    #val,test 예측\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    #val metric\n",
    "    val_MAE=mean_absolute_error(y_val,y_pred_val)\n",
    "    val_RMSE=np.sqrt(mean_squared_error(y_val,y_pred_val))\n",
    "    val_R2=r2_score(y_val,y_pred_val)\n",
    "    val_adj_r2=adj_r2_score(y_val,y_pred_val,X_val)\n",
    "\n",
    "    #test metric\n",
    "    test_MAE=mean_absolute_error(y_test,y_pred_test)\n",
    "    test_RMSE=np.sqrt(mean_squared_error(y_test,y_pred_test))\n",
    "    test_R2=r2_score(y_test,y_pred_test)\n",
    "    test_adj_r2=adj_r2_score(y_test,y_pred_test,X_test)\n",
    "\n",
    "    #val,test DF\n",
    "    val_df=pd.DataFrame({'y_test':y_test,\n",
    "                         'y_pred':y_pred_test,\n",
    "                         'val_MAE':val_MAE,\n",
    "                         'test_MAE':test_MAE,\n",
    "                         'val_RMSE':val_RMSE,\n",
    "                         'test_RMSE':test_RMSE,\n",
    "                         'val_R2':val_R2,\n",
    "                         'test_R2':test_R2,\n",
    "                         'val_adj_R2':val_adj_r2,\n",
    "                         'test_adj_R2':test_adj_r2})\n",
    "    val_df['Model_n']= model_name_str\n",
    "\n",
    "\n",
    "\n",
    "    #visual_df\n",
    "\n",
    "    visual_df=pd.DataFrame({'y_test':y_test,\n",
    "                         'y_pred':y_pred_test})\n",
    "    visual_df['Model_n']= model_name_str\n",
    "    name=model_name_str+'_pred'\n",
    "    globals()[name]=pd.concat([df_test,visual_df],axis=1)\n",
    "    visual_df_name.append(name)\n",
    "\n",
    "    if num == 0:\n",
    "        new_data_df=val_df\n",
    "    else:\n",
    "        new_data_df=pd.concat([new_data_df,val_df])\n",
    "    num+=1\n",
    "for i in range(len(visual_df_name)):\n",
    "    if i==0:\n",
    "        All_new_data=globals()[visual_df_name[i]]\n",
    "    else:\n",
    "        All_new_data=pd.concat([All_new_data,globals()[visual_df_name[i]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOM8kCzhra9g"
   },
   "outputs": [],
   "source": [
    "# #스태킹\n",
    "stack_condition=((result_stack['Data_number']=='X4')&(result_stack['y_test']==result_high_pred))\n",
    "stack_get=result_stack[stack_condition]\n",
    "stack_get=stack_get['best_params'].iloc[0]\n",
    "stack_get = eval(stack_get)\n",
    "meta_model_name = stack_get[0][1]\n",
    "base_mdoel_name = stack_get[1]\n",
    "\n",
    "\n",
    "#메타모델 인스턴스\n",
    "for item_dict in instantiated_models_info:\n",
    "    if item_dict.get('name') == meta_model_name:\n",
    "        found_meta_instance=item_dict['instance']\n",
    "        break\n",
    "\n",
    "#베이스 모델 인스턴스\n",
    "base_model=[]\n",
    "\n",
    "for i in range(len(base_mdoel_name)):\n",
    "    for item_dict in instantiated_models_info:\n",
    "        if item_dict.get('name') == base_mdoel_name[i]:\n",
    "            found_base=item_dict['instance']\n",
    "            base_name=item_dict['name']\n",
    "            base_model.append((base_name,found_base))\n",
    "            break\n",
    "\n",
    "base_model\n",
    "# base_model\n",
    "ladder = StackingRegressor(estimators=base_model,\n",
    "                    final_estimator=found_meta_instance,\n",
    "                    cv=5)\n",
    "\n",
    "ladder.fit(X_train,y_train)\n",
    "\n",
    "y_pred_val = ladder.predict(X_val)\n",
    "y_pred_test = ladder.predict(X_test)\n",
    "\n",
    "#val metric\n",
    "val_MAE=mean_absolute_error(y_val,y_pred_val)\n",
    "val_RMSE=np.sqrt(mean_squared_error(y_val,y_pred_val))\n",
    "val_R2=r2_score(y_val,y_pred_val)\n",
    "val_adj_r2=adj_r2_score(y_val,y_pred_val,X_val)\n",
    "\n",
    "#test metric\n",
    "test_MAE=mean_absolute_error(y_test,y_pred_test)\n",
    "test_RMSE=np.sqrt(mean_squared_error(y_test,y_pred_test))\n",
    "test_R2=r2_score(y_test,y_pred_test)\n",
    "test_adj_r2=adj_r2_score(y_test,y_pred_test,X_test)\n",
    "\n",
    "#val,test DF\n",
    "val_df=pd.DataFrame({'y_test':y_test,\n",
    "                    'y_pred':y_pred_test,\n",
    "                    'val_MAE':val_MAE,\n",
    "                    'test_MAE':test_MAE,\n",
    "                    'val_RMSE':val_RMSE,\n",
    "                    'test_RMSE':test_RMSE,\n",
    "                    'val_R2':val_R2,\n",
    "                    'test_R2':test_R2,\n",
    "                    'val_adj_R2':val_adj_r2,\n",
    "                    'test_adj_R2':test_adj_r2})\n",
    "val_df['Model_n']= 'Stacking'\n",
    "new_data_df=pd.concat([new_data_df,val_df])\n",
    "\n",
    "#visual_df\n",
    "\n",
    "visual_df=pd.DataFrame({'y_test':y_test,\n",
    "                        'y_pred':y_pred_test})\n",
    "visual_df['Model_n']= 'Stacking'\n",
    "name='Stacking'+'_pred'\n",
    "globals()[name]=pd.concat([df_test,visual_df],axis=1)\n",
    "visual_df_name.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG5n_vfkra9g",
    "outputId": "85a35e64-296c-44ac-a3fa-00e26312f83b"
   },
   "outputs": [],
   "source": [
    "#NN\n",
    "best_params = {\n",
    "    'units': 256,\n",
    "    'activation': 'tanh',\n",
    "    'l2': 0.0001,\n",
    "    'optimizer': 'adam',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 16\n",
    "}\n",
    "\n",
    "y_scale=StandardScaler()\n",
    "y_train_2d = y_train.values.reshape(-1, 1)\n",
    "y_train_scaled = y_scale.fit_transform(y_train_2d)\n",
    "\n",
    "\n",
    "# 시드 고정\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# ✅ 베스트 모델 구성\n",
    "def build_model(input_dim, units, activation, l2, optimizer):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(units, activation=activation, input_shape=(input_dim,), kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "        keras.layers.Dense(units//2, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "        keras.layers.Dense(units//4, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2)),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "start_time = time.time()\n",
    "\n",
    "  # 전처리된 베스트 데이터셋 사용\n",
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    units=best_params['units'],\n",
    "    activation=best_params['activation'],\n",
    "    l2=best_params['l2'],\n",
    "    optimizer=best_params['optimizer']\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train_scaled,\n",
    "                    epochs=best_params['epochs'],\n",
    "                    batch_size=best_params['batch_size'],\n",
    "                    verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 예측 및 역변환\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_val= y_scale.inverse_transform(y_pred_val).flatten()\n",
    "y_pred_test = y_scale.inverse_transform(y_pred_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "#val metric\n",
    "val_MAE=mean_absolute_error(y_val,y_pred_val)\n",
    "val_RMSE=np.sqrt(mean_squared_error(y_val,y_pred_val))\n",
    "val_R2=r2_score(y_val,y_pred_val)\n",
    "val_adj_r2=adj_r2_score(y_val,y_pred_val,X_val)\n",
    "\n",
    "#test metric\n",
    "test_MAE=mean_absolute_error(y_test,y_pred_test)\n",
    "test_RMSE=np.sqrt(mean_squared_error(y_test,y_pred_test))\n",
    "test_R2=r2_score(y_test,y_pred_test)\n",
    "test_adj_r2=adj_r2_score(y_test,y_pred_test,X_test)\n",
    "\n",
    "#val,test DF\n",
    "val_df=pd.DataFrame({'y_test':y_test,\n",
    "                    'y_pred':y_pred_test,\n",
    "                    'val_MAE':val_MAE,\n",
    "                    'test_MAE':test_MAE,\n",
    "                    'val_RMSE':val_RMSE,\n",
    "                    'test_RMSE':test_RMSE,\n",
    "                    'val_R2':val_R2,\n",
    "                    'test_R2':test_R2,\n",
    "                    'val_adj_R2':val_adj_r2,\n",
    "                    'test_adj_R2':test_adj_r2})\n",
    "val_df['Model_n']= 'NN'\n",
    "new_data_df=pd.concat([new_data_df,val_df])\n",
    "\n",
    "#visual_df\n",
    "\n",
    "visual_df=pd.DataFrame({'y_test':y_test,\n",
    "                        'y_pred':y_pred_test})\n",
    "visual_df['Model_n']= 'NN'\n",
    "name='NN'+'_pred'\n",
    "globals()[name]=pd.concat([df_test,visual_df],axis=1)\n",
    "visual_df_name.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ4GwQc1ra9h"
   },
   "outputs": [],
   "source": [
    "#Full model set\n",
    "for i in range(len(visual_df_name)):\n",
    "    if i==0:\n",
    "        All_new_data=globals()[visual_df_name[i]]\n",
    "    else:\n",
    "        All_new_data=pd.concat([All_new_data,globals()[visual_df_name[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blVlsUEGra9h"
   },
   "outputs": [],
   "source": [
    "Actual_y = All_new_data.groupby(['Model_n', 'Month'])['y_test'].mean()\n",
    "Predic_y = All_new_data.groupby(['Model_n', 'Month'])['y_pred'].mean()\n",
    "\n",
    "unique_model = Actual_y.index.get_level_values('Model_n').unique()\n",
    "n_model = len(unique_model)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_model / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 15), squeeze=False)\n",
    "fig.suptitle('2024 Year Predict', fontsize=30, fontweight='bold')\n",
    "axes_flat = axes.flatten()\n",
    "last_plotted_idx = -1\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(unique_model):\n",
    "    # 현재 모델이름에 위치하는 ax축 선택\n",
    "    ax = axes_flat[i]\n",
    "    last_plotted_idx = i\n",
    "    actual_data = Actual_y.loc[model_name]\n",
    "    predict_data = Predic_y.loc[model_name]\n",
    "\n",
    "    # 현재 축(ax)에 실제값과 예측값 그래프 그리기\n",
    "    ax.plot(actual_data, label='Actual',marker='o')\n",
    "    ax.plot(predict_data, label='Predicted',  linestyle='--', marker='x')\n",
    "\n",
    "    # 각 서브플롯 꾸미기\n",
    "    ax.set_title(f'Model: {model_name}')      # 플롯 제목 설정\n",
    "    ax.set_xlabel('Month')                    # x축 레이블\n",
    "    ax.set_ylabel('Avg Consumption (kWh)')    # y축 레이블 (단위 확인 필요)\n",
    "    ax.grid(True)                             # 그리드 표시\n",
    "    ax.legend()                               # 범례 표시 확실히 하기\n",
    "\n",
    "for j in range(last_plotted_idx+1, len(axes_flat)):\n",
    "    axes_flat[j].set_visible(False)\n",
    "\n",
    "# 전체 레이아웃 조정 및 플롯 보여주기\n",
    "plt.tight_layout()\n",
    "# tight_layout() 호출 후 suptitle 공간 확보\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlQBHr0Ara9h"
   },
   "source": [
    "### **Time Series Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YM0E1zUPra9h"
   },
   "outputs": [],
   "source": [
    "# Load file\n",
    "df = pd.read_csv('data/cleaned/energy_consumption.csv')\n",
    "df['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))\n",
    "df = df.drop(columns=['Year', 'Month'])\n",
    "df.set_index('Date', inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2JJjM2Lra9h"
   },
   "outputs": [],
   "source": [
    "# Select target\n",
    "y = df['Avg Electricity Consumption per Household (kWh)']\n",
    "y = y.resample('MS').mean()\n",
    "title = 'Avg Electricity Consumption per Household (kWh)'\n",
    "\n",
    "# y = df['Avg Gas Supply per Household (ton)']\n",
    "# y = y.resample('MS').mean()\n",
    "# title = 'Avg Gas Supply per Household (ton)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4ZG7Sycra9h",
    "outputId": "4d7db951-706a-463d-a763-13c790a73983"
   },
   "outputs": [],
   "source": [
    "# Check stationarity using Augmented Dickey-Fuller test\n",
    "adf = adfuller(y.dropna())\n",
    "print(f\"ADF stats: {adf[0]} | p-val: {adf[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bnO6UDPra9h"
   },
   "outputs": [],
   "source": [
    "# Split into train and test data\n",
    "date = '2023-01-01'\n",
    "y_train, y_test = y[:date], y[date:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4siYp1xUra9h",
    "outputId": "622c3ab5-8579-4ce7-d84c-b8b31a7d39f5"
   },
   "outputs": [],
   "source": [
    "# SARIMAX\n",
    "sarimax = SARIMAX(y_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "sarimax_fit = sarimax.fit()\n",
    "print(sarimax_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUbK2DyGra9h"
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = sarimax_fit.get_prediction(start=y_test.index[0], end=y_test.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ztN6doora9h",
    "outputId": "b7986d9b-14c5-4590-9304-051be76fd437"
   },
   "outputs": [],
   "source": [
    "# Forecast\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(y_train, label='Train')\n",
    "plt.plot(y_test, label='Test')\n",
    "plt.plot(y_pred.predited_mean, label='Prediction', linestyle='--')c\n",
    "plt.legend()\n",
    "plt.title(f\"Prediction of {title} using SARIMAX\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
