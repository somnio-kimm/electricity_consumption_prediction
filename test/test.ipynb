{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/homebrew/anaconda3/envs/venv_3.11/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/homebrew/anaconda3/envs/venv_3.11/lib/python3.11/site-packages/hyperopt/atpe.py:19: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "# Time\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# File\n",
    "import warnings\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import chardet\n",
    "import itertools\n",
    "\n",
    "# Numerical & Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import math\n",
    "from typing import List, Callable, Union, Dict, Any, Tuple\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "from sklearn.tree import plot_tree\n",
    "from scipy.optimize import curve_fit\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor, \n",
    "\t\t\t\t\t\t\t  ExtraTreesClassifier, ExtraTreesRegressor, \n",
    "\t\t\t\t\t\t\t  BaggingClassifier, BaggingRegressor, \n",
    "\t\t\t\t\t\t\t  GradientBoostingClassifier, GradientBoostingRegressor, \n",
    "\t\t\t\t\t\t\t  AdaBoostClassifier, AdaBoostRegressor, \n",
    "\t\t\t\t\t\t\t  VotingClassifier, VotingRegressor,\n",
    "\t\t\t\t\t\t\t  StackingClassifier, StackingRegressor)\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Neural Network Libraries\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor\n",
    "import huggingface\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import (StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer, \n",
    "\t\t\t\t\t\t\t\t   LabelEncoder, OneHotEncoder, OrdinalEncoder, LabelBinarizer)\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, RFE, SequentialFeatureSelector, VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import SimpleImputer\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Time-Series Analysis\n",
    "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (classification_report, pairwise_distances, silhouette_score, \n",
    "\t\t\t\t\t\t\t roc_curve, auc, roc_auc_score, RocCurveDisplay, \n",
    "\t\t\t\t\t\t\t confusion_matrix, ConfusionMatrixDisplay, \n",
    "\t\t\t\t\t\t\t accuracy_score, recall_score, precision_score, f1_score,\n",
    "\t\t\t\t\t\t\t log_loss, hinge_loss, mean_absolute_error, mean_squared_error, r2_score)\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC, ConfusionMatrix, MeanSquaredError, MeanAbsoluteError, R2Score, MetricCollection\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly average electricity consumption + temperture\n",
    "\n",
    "avg_t_mean = df.groupby('Month')['Avg Temperature (Celsius)'].mean()\n",
    "avg_pc_mean = df.groupby('Month')['Avg Electricity Consumption per Household (kWh)'].mean()\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.set_facecolor('white') \n",
    "ax = fig.add_subplot()\n",
    " \n",
    "ax.bar(avg_t_mean.index, avg_t_mean , label='Avg Temperature (Celsius)')\n",
    "ax.set_ylabel('Avg Temperature (Celsius)', fontsize=14, color='black') \n",
    "ax.set_xlabel('Month', fontsize=14, color='black')\n",
    " \n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel('Avg Electricity Consumption per Household (kWh)', fontsize=14, color='red')\n",
    "ax2.plot(avg_pc_mean.index ,avg_pc_mean, label='Avg Electricity Consumption per Household (kWh)',color='red', linewidth = 5)\n",
    "\n",
    "plt.xticks(np.arange(0,13,1),rotation=0)\n",
    "plt.title('Avg Temperature & Avg Electricity Consumption per Household (kWh)',fontsize=20)\n",
    "plt.legend(loc = 2, fontsize=20, frameon=True, shadow=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Year per: 84 Month per: 70 Region per: 140 Total data:840\n",
    "# \n",
    "# monthly_data = df.groupby('Region')['Avg Electricity Consumption per Household (kWh) Household (kWh)'].mean( ).reset_index()\n",
    "# df_mean=df.groupby(['YeaAvg Electricity Consumption per Household (kWh)r Househol# d (kWh)']).mean()\n",
    "# unstacked_df = df_mean.unst# ack(level='Year')\n",
    "# fig, ax_1 = plt.subplot# s(figsize=(12, 8))\n",
    "# ax_1.set# _xlabel('Region')\n",
    "# ax_1.set_xticks# (monthly_data.indAvg Electricity Consumption per Household (kWh)on # per Household (kWh)')\n",
    "# ax_1.plot(monthly_data['LocAvg Electricity Consumption per Household (kWh)'on per Household (kWh)'], color='orange',mark# er='o', ms=4,alpha=1)\n",
    "\n",
    "# ax_1.tick_params(axis='# y', labelcolor='orange')\n",
    "# ### #####\n",
    "# ax_2 = ax_Small Pan Evaporation (mm)('S# mall Pan Evaporation (mm)')\n",
    "# unstacked_df.plot(k# ind='bar',ax=ax_2,alpha=0.5)\n",
    "# ax_2.tick_params(# axis='y', labelcolor='blue')\n",
    "\n",
    "# plt.title(f'Avg Electricity Consumption per Household#  (kWh) from#  Region,Region'# )\n",
    "# plt.grid()\n",
    "# plt.tight_layout()\n",
    "# for x_val,y_val in zip(monthly_data['Region'],monthly_data['Avg Electricity C# onsumption per Household (kWh)']):\n",
    "# \tax_1.text(x_val, y_val +0.1,f'{y_val:.2f}',ha='center',va='bottom'# ,fontsize=9,color='black',zorde# r=5)\n",
    "# plt.subplots_adjust(right=1.8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(models: List[Union[GridSearchCV, BaseEstimator]], X_train_list: List[pd.DataFrame], X_test_list: List[pd.DataFrame], y_train: pd.Series, y_test: pd.Series) -> pd.DataFrame:\n",
    "\t\"\"\"\n",
    "\tEvaluates a list of models (either GridSearchCV objects or regular estimators)\n",
    "\ton training and test data, and returns a summary DataFrame of evaluation metrics.\n",
    "\n",
    "\tParameters:\n",
    "\t\tmodels (List[Union[GridSearchCV, BaseEstimator]]): List of models to evaluate.\n",
    "\t\tX_train_list (List[pd.DataFrame]): Training features.\n",
    "\t\tX_test_list (List[pd.DataFrame]): Testing features.\n",
    "\t\ty_train (pd.Series): Training labels.\n",
    "\t\ty_test (pd.Series): Testing labels.\n",
    "\n",
    "\tReturns:\n",
    "\t\tpd.DataFrame: A summary table with model name, evaluation metrics,\n",
    "\t\t\t\t\t  training time, and best hyperparameters (if available).\n",
    "\t\"\"\"\n",
    "\tresults = []\n",
    "\t\n",
    "\tfor i, (X_train, X_test) in enumerate(zip(X_train_list, X_test_list)):\n",
    "\t\tfor model in models:\n",
    "\t\t\t# Training for each X_train\n",
    "\t\t\tmodel_clone = clone(model)\n",
    "\t\t\tmodel_clone.fit(X_train, y_train)\n",
    "\t\t\t\n",
    "\t\t\t# Check if a model is a GridSearchCV object\n",
    "\t\t\tif isinstance(model_clone, GridSearchCV):\n",
    "\t\t\t\tbest_model = model_clone.best_estimator_\n",
    "\t\t\t\tbest_hyperparams = model_clone.best_params_\n",
    "\t\t\telse:\n",
    "\t\t\t\tbest_model = model\n",
    "\t\t\t\tbest_hyperparams = 'N/A'\n",
    "\t\t\tmodel_name = str(best_model.__class__.__name__)\n",
    "\t\t\t\n",
    "\t\t\t# Testing for each X_test\n",
    "\t\t\ty_pred = best_model.predict(X_test)\n",
    "\t\t\t\n",
    "\t\t\t# Evaluation\n",
    "\t\t\tmae = mean_absolute_error(y_test,y_pred)\n",
    "\t\t\tmse = mean_squared_error(y_test, y_pred)\n",
    "\t\t\trmse = np.sqrt(mse)\n",
    "\t\t\tr2 = r2_score(y_test,y_pred)\n",
    "\t\t\tadj_r2 = adj_r2_score(y_test,y_pred,X_test)\n",
    "\t\t\t\n",
    "\t\t\t# Result dataframe\n",
    "\t\t\tresult = pd.DataFrame({'Feature Index': [i],\n",
    "\t\t\t\t\t\t\t\t   'Model': [model_name],\n",
    "\t\t\t\t\t\t\t\t   'MAE': [mae],\n",
    "\t\t\t\t\t\t\t\t   'RMSE': [rmse],\n",
    "\t\t\t\t\t\t\t\t   'R^2': [r2],\n",
    "\t\t\t\t\t\t\t\t   'Adj-R^2': [adj_r2],\n",
    "\t\t\t\t\t\t\t\t   'Training time': [training_time],\n",
    "\t\t\t\t\t\t\t\t   'Best hyperparameters': [best_hyperparams]})\n",
    "\t\t\tresults.append(result)\n",
    "\t\t\t\n",
    "\t\t# Sort results by Model\n",
    "\t\tresults = pd.concat(results).sort_values(by=['Feature Index', 'Model']).reset_index(drop=True)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "models = [model_lr, model_en, model_dt, model_svr, model_rf, model_ada, model_xgb, model_stacking]\n",
    "results = evaluation(models=models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains=[X1_train,X2_train,X3_train,X4_train,X5_train]\n",
    "X_tests=[X1_test,X2_test,X3_test,X4_test,X5_test]\n",
    "dummy_prefix = 'Location'\n",
    "dummy_sep = '_'\n",
    "for i, (train_df, test_df) in enumerate(zip(X_trains, X_tests)):\n",
    "    print(f\"  ì²˜ë¦¬ ì¤‘: {i+1}ë²ˆì§¸ Train/Test ìŒ\") # i+1 í•˜ë©´ 1, 2, 3, 4, 5 ê°€ ë©ë‹ˆë‹¤.\n",
    "    train_name=f'X{i+1}_train'\n",
    "    test_name=f'X{i+1}_test'\n",
    "    train_processed = pd.get_dummies(train_df, columns=['Location'],dtype=int, prefix=dummy_prefix, prefix_sep=dummy_sep)\n",
    "    test_processed = pd.get_dummies(test_df, columns=['Location'],dtype=int, prefix=dummy_prefix, prefix_sep=dummy_sep)\n",
    "    all_cols = list(set(train_processed.columns) | set(test_processed.columns))\n",
    "    train_reindexed = train_processed.reindex(columns=all_cols, fill_value=0)\n",
    "    test_reindexed = test_processed.reindex(columns=all_cols, fill_value=0)\n",
    "    # ì›-í•« ì¸ì½”ë”© ì‹œ ì‚¬ìš©í•œ prefixë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë”ë¯¸ ì»¬ëŸ¼ ì‹ë³„\n",
    "    dummy_cols = [col for col in train_reindexed.columns if col.startswith(f\"{dummy_prefix}{dummy_sep}\")]\n",
    "    # ë”ë¯¸ ì»¬ëŸ¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ë¥¼ ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ìˆ«ìí˜• ì»¬ëŸ¼ìœ¼ë¡œ ê°„ì£¼\n",
    "    numerical_cols = [col for col in train_reindexed.columns if col not in dummy_cols]\n",
    "    scaler=StandardScaler()\n",
    "    scaler.fit(train_reindexed[numerical_cols])\n",
    "    scaled_train_numerical_np = scaler.transform(train_reindexed[numerical_cols])\n",
    "    train_scaled_numerical_df = pd.DataFrame(scaled_train_numerical_np,\n",
    "                                             index=train_reindexed.index,\n",
    "                                             columns=numerical_cols)\n",
    "\n",
    "    scaled_test_numerical_np = scaler.transform(test_reindexed[numerical_cols])\n",
    "    test_scaled_numerical_df = pd.DataFrame(scaled_test_numerical_np,\n",
    "                                            index=test_reindexed.index,\n",
    "                                            columns=numerical_cols)\n",
    "    final_train_df = pd.concat([train_scaled_numerical_df, train_reindexed[dummy_cols]], axis=1)\n",
    "    final_test_df = pd.concat([test_scaled_numerical_df, test_reindexed[dummy_cols]], axis=1)\n",
    "\n",
    "    print(f\"    ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ì»¬ëŸ¼ (Numerical): {numerical_cols}\")\n",
    "    print(f\"    ìœ ì§€ ëŒ€ìƒ ì»¬ëŸ¼ (Dummy): {dummy_cols}\")\n",
    "    globals()[train_name] = final_train_df\n",
    "    globals()[test_name] = final_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(X, y,\n",
    "\t\t\t\t\t   initial_list=[],\n",
    "\t\t\t\t\t   threshold_in=0.05,\n",
    "\t\t\t\t\t   threshold_out=0.10,\n",
    "\t\t\t\t\t   verbose=True):\n",
    "\t\"\"\"\n",
    "\tPerform a stepwise feature selection based on p-values from statsmodels OLS.\n",
    "\n",
    "\tParameters:\n",
    "\t- X : pd.DataFrame\n",
    "\t\tCandidate feature set (independent variables).\n",
    "\t- y : pd.Series or np.array\n",
    "\t\tTarget variable (dependent variable).\n",
    "\t- initial_list : list\n",
    "\t\tInitial list of features to start the selection process.\n",
    "\t- threshold_in : float\n",
    "\t\tp-value threshold for adding a feature (smaller = more strict).\n",
    "\t- threshold_out : float\n",
    "\t\tp-value threshold for removing a feature (larger = more lenient).\n",
    "\t- verbose : bool\n",
    "\t\tWhether to print progress during feature selection.\n",
    "\n",
    "\tReturns:\n",
    "\t- included : list\n",
    "\t\tThe final list of selected features.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tincluded = list(initial_list)  # Start with an initial list (could be empty)\n",
    "\t\n",
    "\twhile True:\n",
    "\t\tchanged = False  # Flag to track whether any feature was added or removed in the current iteration; If not, the loop will break\n",
    "\n",
    "\t\t# --- Forward Step ---\n",
    "\t\t# Try adding each feature not yet included and check p-values\n",
    "\t\texcluded = list(set(X.columns) - set(included))  # Compute the list of features not yet included in the model; These are the candidates for addition\n",
    "\t\tnew_pval = pd.Series(index=excluded, dtype=float)  # Initialise a Series to store the p-values of each excluded feature if it were to be added to the model\n",
    "\t\tfor new_column in excluded: # Iterate over all excluded features to assess their contribution\n",
    "\t\t\t# Fit OLS model with the current included features + this new one\n",
    "\t\t\tX_with_const = sm.add_constant(X[included + [new_column]]) # Prepare the design matrix with a constant term (intercept) and the current included features plus the candidate new feature\n",
    "\t\t\tmodel = sm.OLS(y, X_with_const).fit() # Fits an Ordinary Least Squares (OLS) linear regression model to the current design matrix\n",
    "\t\t\tnew_pval[new_column] = model.pvalues[new_column]  # Extract the p-value of the newly added feature and stores it\n",
    "\n",
    "\t\t# Add the feature with the lowest p-value if it's below threshold_in\n",
    "\t\tif not new_pval.empty and new_pval.min() < threshold_in: # Check whether the smallest p-value among the excluded features is statistically significant, i.e., below the inclusion threshold\n",
    "\t\t\tbest_pval = new_pval.idxmin()  # Feature with the smallest p-value\n",
    "\t\t\tincluded.append(best_pval)\n",
    "\t\t\tchanged = True\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f'Add {best_pval} with p-value {new_pval.min():.6f}')\n",
    "\n",
    "\t\t# --- Backward Step ---\n",
    "\t\t# Now check if any included feature should be removed\n",
    "\t\tX_with_const = sm.add_constant(X[included]) \n",
    "\t\tmodel = sm.OLS(y, X_with_const).fit() # Re-fit the model using the current set of included features to re-calculate all p-values\n",
    "\t\tpvalues = model.pvalues.iloc[1:]  # Get p-values for all features excluding the intercept (which is the first value); These are the features being evaluated for possible removal\n",
    "\n",
    "\t\t# If any included feature has a p-value above threshold_out, remove the worst one\n",
    "\t\tif not pvalues.empty and pvalues.max() > threshold_out: # If the worst (largest) p-value among included features exceeds the exclusion threshold, itâ€™s a candidate for removal\n",
    "\t\t\tworst_pval = pvalues.idxmax() # Find the feature with the worst (largest) p-value\n",
    "\t\t\tincluded.remove(worst_pval) # Remove this feature from the model\n",
    "\t\t\tchanged = True\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f'Drop {worst_pval} with p-value {pvalues.max():.6f}')\n",
    "\n",
    "\t\t# If no feature was added or removed, the process is done\n",
    "\t\tif not changed:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Print final selected features and summary\n",
    "\tif verbose:\n",
    "\t\tprint(\"\\nFinal Selected Variables:\")\n",
    "\t\tprint(included)\n",
    "\t\tfinal_X_with_const = sm.add_constant(X[included])\n",
    "\t\tfinal_model = sm.OLS(y, final_X_with_const).fit()\n",
    "\t\tprint(\"\\nFinal Model Summary:\")\n",
    "\t\tprint(final_model.summary())\n",
    "\n",
    "\treturn included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_corr_col(X: pd.DataFrame, Y: pd.Series, feature: str, threshold: float):\n",
    "\t\"\"\"\n",
    "\tIdentify numeric columns that are weakly correlated with a specified feature.\n",
    "\n",
    "\tParameters:\n",
    "\t- X (pd.DataFrame): Feature dataset.\n",
    "\t- Y (pd.Series): Target variable to be appended for correlation analysis.\n",
    "\t- feature (str): The column name to compare correlations against.\n",
    "\t- threshold (float): Absolute correlation threshold; columns with correlation\n",
    "\t\t\t\t\t\t less than this value will be returned.\n",
    "\n",
    "\tReturns:\n",
    "\t- pd.Index: Column names with absolute correlation to the specified feature\n",
    "\t\t\t\tless than the given threshold.\n",
    "\t\"\"\"\n",
    "\tdf = pd.concat([X, Y], axis=1)\n",
    "\tdf_numeric = df.select_dtypes(include=['number'])\n",
    "\tdf_corr = df_numeric.corr()\n",
    "\tweak_corr_cols = df_corr[abs(df_corr[feature]) < threshold].index\n",
    "\t\n",
    "\treturn weak_corr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_r2_score(y_true: pd.Series, y_pred: pd.Series, X_features: pd.DataFrame) -> float:\n",
    "\t\"\"\"\n",
    "\tCompute the adjusted RÂ² (coefficient of determination).\n",
    "\n",
    "\tAdjusted RÂ² adjusts the regular RÂ² score for the number of predictors (features)\n",
    "\tin the model. It penalizes the RÂ² score for adding unnecessary predictors, helping\n",
    "\tto avoid overfitting.\n",
    "\n",
    "\tParameters:\n",
    "\t- y_true : array-like of shape (n_samples,)\n",
    "\t\tTrue target values.\n",
    "\t- y_pred : array-like of shape (n_samples,)\n",
    "\t\tPredicted target values from the model.\n",
    "\t- X_features : array-like of shape (n_samples, n_features)\n",
    "\t\tData used for getting the number of predictors (features) used in the model.\n",
    "\n",
    "\tReturns:\n",
    "\t- adj_r2 : float\n",
    "\t\tThe adjusted R-squared score. Returns NaN if the number of predictors is too large\n",
    "\t\tfor the formula to be valid (i.e., n <= p + 1).\n",
    "\t\"\"\"\n",
    "\tn = len(y_true)\n",
    "\tp = X_features.shape[1]\n",
    "\t\n",
    "\t# Prevent division by zero or negative denominator\n",
    "\tif n <= p + 1:\n",
    "\t\twarnings.warn(\"Adjust R^2 is undefined.\")\n",
    "\t\treturn float('nan')\n",
    "\t\n",
    "\treturn 1 - (1 - r2_score(y_true, y_pred)) * (n - 1) / (n - p - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: BaseEstimator,  \n",
    "\t\t\t\t   X_test: pd.DataFrame, \n",
    "\t\t\t\t   y_test: pd.Series) -> Tuple[pd.Series, float, float, float, float]:\n",
    "\t\"\"\"\n",
    "\tTrain a regression model and evaluates its performance on the test set.\n",
    "\n",
    "\tParameters:\n",
    "\t\tmodel (BaseEstimator): The regression model to train (already instantiated, e.g., from GridSearchCV.best_estimator_).\n",
    "\t\tX_test (pd.DataFrame): Testing features.\n",
    "\t\ty_test (pd.Series): Testing target values.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple containing:\n",
    "\t\t\t- y_pred (pd.Series): Predicted target values on the test set.\n",
    "\t\t\t- mae (float): Mean Absolute Error.\n",
    "\t\t\t- rmse (float): Root Mean Squared Error.\n",
    "\t\t\t- r2 (float): R-squared score.\n",
    "\t\t\t- adj_r2 (float): Adjusted R-squared score.\n",
    "\t\"\"\"\n",
    "\ty_pred = model.predict(X_test)\n",
    "\tmae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
    "\tmse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "\trmse = math.sqrt(mse)\n",
    "\tr2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "\tadj_r2 = adj_r2_score(y_true=y_test, y_pred=y_pred, X_features=X_test)\n",
    "\t\n",
    "\treturn y_pred, mae, rmse, r2, adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/cleaned/energy_consumption.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split for features and target\n",
    "X = df.drop(columns=['Avg Electricity Consumption per Household (kWh)', 'Avg Gas Supply per Household (ton)'])\n",
    "y = df['Avg Electricity Consumption per Household (kWh)']\n",
    "# y = df['Avg Gas Supply per Household (ton)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab column names of each data type\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "datetime_cols = X.select_dtypes(include=['datetime']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split for train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region one hot encoding\n",
    "X_train = pd.get_dummies(data=X_train, columns=['Region'], dtype=int)\n",
    "X_test = pd.get_dummies(data=X_test, columns=['Region'], dtype=int)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X0 - All\n",
    "X0_train = X_train.copy()\n",
    "X0_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 - Correlation\n",
    "weak_corr_cols = weak_corr_col(X_train, y_train,'Avg Electricity Consumption per Household (kWh)', 0.2)\n",
    "X1_train = X_train.drop(columns=(weak_corr_cols))   \n",
    "X1_test = X_test.drop(columns=(weak_corr_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config setting\n",
    "# Elastic net\n",
    "model_en = ElasticNet(random_state=42)\n",
    "hyperparams_en = {'alpha': [0.1, 1, 10],\n",
    "\t\t\t\t  'l1_ratio': [0.1, 0.5, 0.9]}                          \n",
    "config_en = [model_en, hyperparams_en]\n",
    "\n",
    "# SVR\n",
    "model_svr = SVR()\n",
    "hyperparams_svr = {'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "\t\t\t\t   'C':[10, 100, 1000],\n",
    "\t\t\t\t   'epsilon':[0.3,0.5,0.8]}\n",
    "config_svr = [model_svr, hyperparams_svr]\n",
    "\n",
    "# XGB\n",
    "model_xgb = XGBRegressor(random_state=42)\n",
    "hyperparams_xgb = {'n_estimators': [100, 200, 300],\n",
    "\t\t\t\t   'learning_rate': [0.01, 0.05, 0.1],\n",
    "\t\t\t\t   'max_depth': [3, 5, 7]}\n",
    "config_xgb = [model_xgb, hyperparams_xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(models_dict: Dict[str, Any], max_k:int, min_k: int) -> List[Tuple[Tuple[str, Any], ...]]:\n",
    "\t\"\"\"\n",
    "    Generate all possible combinations of base learners from a dictionary of models,\n",
    "    with combination sizes ranging from min_k to max_k.\n",
    "\n",
    "    Parameters:\n",
    "    models_dict : dict\n",
    "        A dictionary where keys are model names and values are trained model objects.\n",
    "        Example: {'ElasticNet': model_1, 'XGBRegressor': model_2}\n",
    "\n",
    "    min_k : int, optional (default=2)\n",
    "        The minimum number of base models in a combination.\n",
    "\n",
    "    max_k : int, optional (default=6)\n",
    "        The maximum number of base models in a combination.\n",
    "\n",
    "    Returns:\n",
    "    all_combos : list of tuples\n",
    "        Each tuple contains a combination of (model_name, model_object) pairs.\n",
    "        Example: [(('ElasticNet', model1), ('XGBRegressor', model2)), ...]\n",
    "    \"\"\"\n",
    "\t# Convert the dictionary into a list of (key, value) tuples\n",
    "\titems = list(models_dict.items())\n",
    "\t\n",
    "\t# Initialisation\n",
    "\tcombinations = []\n",
    "\t\n",
    "\t# Ensure max_k does not exceed the number of available models\n",
    "\tmax_k = min(max_k, len(items))\n",
    "\t\n",
    "\t# Generate combinations of size k from min_k to max_k\n",
    "\tfor k in range(min_k, max_k + 1):\n",
    "\t\tcombos = itertools.combinations(items, k)\n",
    "\t\tfor combo in combos:\n",
    "\t\t\tcombinations.append(combo)\n",
    "\t\t\t\n",
    "\treturn combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and X to test\n",
    "models = [config_linear_regression, config_en, config_svr, config_dt, config_rf, config_ada, config_xgb]\n",
    "X_trains=[X0_train, X1_train, X2_train, X3_train, X4_train]\n",
    "X_tests=[X0_test, X1_test, X2_test, X3_test, X4_test]\n",
    "\n",
    "# Initialisation\n",
    "best_estimators_dict = {}\n",
    "best_params_dict = {}\n",
    "modelling_results = []\n",
    "stacking_results = []\n",
    "feature_importances = []\n",
    "\n",
    "# Iterate over each feature (X0, X1, ...)\n",
    "for data_idx, (X_train, X_test) in enumerate(zip(X_trains, X_tests)):\n",
    "\tdata_tag = f'X{data_idx}' # Label for current dataset\n",
    "\tprint(f\"Running on {data_tag}\")\n",
    "\t\n",
    "\tbest_estimators_dict[data_tag] = {}\n",
    "\tbest_params_dict[data_tag] = {}\n",
    "\t\n",
    "\t# Iterate over all models\n",
    "\tfor model_idx, (estimator, param_grid) in enumerate(models):\n",
    "\t\tmodel_name = estimator.__class__.__name__\n",
    "\t\t\n",
    "\t\t# Training\n",
    "\t\tgrid_search = GridSearchCV(estimator=estimator,\n",
    "\t\t\t\t    \t\t\t   param_grid=param_grid,\n",
    "\t\t\t\t\t\t\t\t   cv=5)\n",
    "\t\tstart_time = time.time()\n",
    "\t\tgrid_search.fit(X_train, y_train)\n",
    "\t\tend_time = time.time()\n",
    "\t\ttraining_time = end_time - start_time\n",
    "\t\t\n",
    "\t\t# Store the best model and its hyperparameters\n",
    "\t\tbest_model = grid_search.best_estimator_\n",
    "\t\tbest_params = grid_search.best_params_\n",
    "\t\tbest_estimators_dict[data_tag][model_name] = best_model\n",
    "\t\tbest_params_dict[data_tag][model_name] = best_params\n",
    "\t\t\n",
    "\t\t# Testing and evaluating\n",
    "\t\ty_pred, mae, rmse, r2, adj_r2 = evaluate_model(best_model, X_test, y_test)\n",
    "\t\t\n",
    "\t\t# Log result\n",
    "\t\tmodelling_results.append({'y_test':y_test,\n",
    "\t\t\t\t\t\t\t\t  'y_pred':y_pred,\n",
    "\t\t\t\t\t\t\t\t  'Data': data_tag,\n",
    "\t\t\t\t\t\t\t\t  'Model': model_name,\n",
    "\t\t\t\t\t\t\t\t  'Training time': training_time,\n",
    "\t\t\t\t\t\t\t\t  'Best hyperparams': best_params,\n",
    "\t\t\t\t\t\t\t\t  'MAE': mae,\n",
    "\t\t\t\t\t\t\t\t  'RMSE': rmse,\n",
    "\t\t\t\t\t\t\t\t  'R^2': r2,\n",
    "\t\t\t\t\t\t\t\t  'Adjusted R^2': adj_r2})\n",
    "\t\n",
    "\t# Stacking\n",
    "\testimators = best_estimators_dict[data_tag]\n",
    "\t\n",
    "\t# Rank models by adjusted R^2\n",
    "\tdf_results = (pd.DataFrame([result for result in modelling_results if result['Data']==data_tag])\n",
    "\t\t\t\t  .sort_values(by='Adjusted R^2', ascending=False)\n",
    "\t\t\t\t  .reset_index(drop=True))\n",
    "\t\n",
    "\t# Select the meta learner w/ top performances\n",
    "\tmeta_learner_name = df_results.iloc[0]['Model']\n",
    "\tmeta_model = estimators[meta_learner_name]\n",
    "\t\n",
    "\t# Select next best models as base learners\n",
    "\tbase_candidates = df_results.iloc[1:]['Model'].tolist()\n",
    "\tbase_objects = {name: estimators[name] for name in base_candidates}\n",
    "\t\n",
    "\tbest_adj_r2 = 0\n",
    "\tbest_combo = None\n",
    "\tbest_stacking = None\n",
    "\t\n",
    "\t# print(f\"[{data_tag}] Meta Learner: {meta_learner_name}\")\n",
    "\t# print(f\"[{data_tag}] Base candidates: {list(base_objects.keys())}\")\n",
    "\t\n",
    "\t# Try all combinations of base learners\n",
    "\tfor combo in generate_combinations(base_objects, max_k=6, min_k=1):\n",
    "\t\t# print(f\"[{data_tag}] Trying combo: {combo}\")\n",
    "\t\tcombo_names, estimators_combo = zip(*combo)\n",
    "\t\t\n",
    "\t\t# Build the stacking model\n",
    "\t\tstack_model = StackingRegressor(estimators=list(zip(combo_names, estimators_combo)),\n",
    "\t\t\t\t\t\t\t\t\t\tfinal_estimator=meta_model,\n",
    "\t\t\t\t\t\t\t\t\t\tcv=5,\n",
    "\t\t\t\t\t\t\t\t\t\tn_jobs=-1)\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\t# Training and testing\n",
    "\t\t\tstack_model.fit(X_train, y_train)\n",
    "\t\t\ty_pred, mae, rmse, r2, adj_r2 = evaluate_model(stack_model, X_test, y_test)\n",
    "\t\t\t\n",
    "\t\t\t# Update adjusted R^2 score\n",
    "\t\t\tif adj_r2 > best_adj_r2:\n",
    "\t\t\t\tbest_adj_r2 = adj_r2\n",
    "\t\t\t\tbest_combo = combo_names\n",
    "\t\t\t\tbest_stacking = stack_model\n",
    "\t\t\t\t# print(f\">>> New best adjusted R^2: {best_adj_r2}\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error during stacking with combination {combo_names}: {e}\")\n",
    "\t\n",
    "\t# Log the best stacking model if one was found\n",
    "\tif best_stacking:\n",
    "\t\ty_pred, mae, rmse, r2, adj_r2 = evaluate_model(best_stacking, X_test, y_test)\n",
    "\t\t\n",
    "\t\tstacking_results.append({'y_test':y_test,\n",
    "\t\t\t\t\t\t   \t\t 'y_pred':y_pred,\n",
    "\t\t\t\t\t\t\t\t 'Data': data_tag,\n",
    "\t\t\t\t\t\t\t\t 'Model': 'Stacking',\n",
    "\t\t\t\t\t\t\t\t 'Training time': 'N/A',\n",
    "\t\t\t\t\t\t\t\t 'Best hyperparams': {'Meta': meta_learner_name, 'Base': list(best_combo)},\n",
    "\t\t\t\t\t\t\t\t 'MAE': mae,\n",
    "\t\t\t\t\t\t\t\t 'RMSE': rmse,\n",
    "\t\t\t\t\t\t\t\t 'R^2': r2,\n",
    "\t\t\t\t\t\t\t\t 'Adjusted R^2': adj_r2})\n",
    "\t\t\n",
    "\t\t# Compute permutation importance\n",
    "\t\tresult = permutation_importance(best_stacking,\n",
    "\t\t\t\t\t\t\t\t\t\tX_test, \n",
    "\t\t\t\t\t\t\t\t\t\ty_test, \n",
    "\t\t\t\t\t\t\t\t\t\tn_repeats=10, \n",
    "\t\t\t\t\t\t\t\t\t\trandom_state=42, \n",
    "\t\t\t\t\t\t\t\t\t\tn_jobs=-1, \n",
    "\t\t\t\t\t\t\t\t\t\tscoring='r2')\n",
    "\t\t\n",
    "\t\tfeature_names = X_test.columns if isinstance(X_test, pd.DataFrame) else [f'Feature {i}' for i in range(X_test.shape[1])]\n",
    "\t\t\n",
    "\t\tdf_perm = (pd.DataFrame({'Data': data_tag,\n",
    "\t\t\t\t\t\t\t\t 'Feature': feature_names,\n",
    "\t\t\t\t\t\t\t\t 'Mean importance': result.importances_mean,\n",
    "\t\t\t\t\t\t\t\t 'Std importance': result.importances_std})\n",
    "\t\t\t\t   .sort_values(by='Mean importance', ascending=False)\n",
    "\t\t\t\t   .reset_index(drop=True))\n",
    "\t\t\n",
    "\t\tfeature_importances.append(df_perm)\n",
    "\n",
    "# Final DataFrame\t\t\t\n",
    "df_modelling = pd.DataFrame(modelling_results)\n",
    "df_stacking = pd.DataFrame(stacking_results)\n",
    "df_models = pd.concat([df_modelling, df_stacking])\n",
    "df_feature_importance = pd.concat(feature_importances, ignore_index=True)\n",
    "\n",
    "# print(\"\\n Model performance summary:\")\n",
    "# print(df_modelling)\n",
    "\n",
    "# print(\"\\n Stacking performance summary:\")\n",
    "# print(df_stacking)\n",
    "\n",
    "# print(\"\\n Feature importance from stacking models:\")\n",
    "# print(df_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch ëª¨ë¸ì— ë“¤ì–´ê°ˆ\n",
    "models = [config_linear_regression, config_en, config_svr, config_dt, config_rf, config_ada, config_xgb]\n",
    "# models = [config_en, config_svr, config_dt, config_ada, config_xgb]\n",
    "num=0\n",
    "n=0\n",
    "#X_trains,X_testsëŠ” Feature Selection ê·¸ë£¹\n",
    "X_trains=[X0_train, X1_train, X2_train, X3_train, X4_train]\n",
    "X_tests=[X0_test, X1_test, X2_test, X3_test, X4_test]\n",
    "\n",
    "\n",
    "for X_train, X_test in zip(X_trains,X_tests):\n",
    "    #DFì— ë“¤ì–´ê°ˆ íŠ¹ì„±ê·¸ë£¹ ë²ˆí˜¸\n",
    "    Data_num='X'+str(num+1)\n",
    "    #í˜„ì¬ ëŒì•„ê°€ê³  ìˆëŠ” ìœ„ì¹˜\n",
    "    print(num)\n",
    "    for i in range(len(models)):\n",
    "        #í˜„ì¬ ëŒì•„ê°€ê³ ìˆëŠ” ìœ„ì¹˜ 2\n",
    "        print(n)\n",
    "        #Model-> ëª¨ë¸ ì„ ì–¸ ex)SVR(),XGBRegressor()\n",
    "        Model=models[i][0]\n",
    "        #Hyperparams-> Hyperparamì˜ íŠœë‹ ëª¨ìŒ\n",
    "        Hyperparams=models[i][1]\n",
    "\n",
    "        #Model,Hyperparamsë¥¼ í†µí•´ GridSearch í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "        model_info = GridSearchCV(estimator=Model,\n",
    "                            param_grid = Hyperparams,\n",
    "                            cv=5)\n",
    "        start_time = time.time()\n",
    "        #GridSearch í›ˆë ¨\n",
    "        model_info.fit(X_train,y_train)\n",
    "        end_time = time.time()\n",
    "        #model_n:ê° ëª¨ë¸ì˜ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸\n",
    "        #ex)XGBRegressor(n_estimators:300,learning_rate:0.1,max_depth:5)\n",
    "        #ìœ„ ëª¨ë¸ì´ ì„±ëŠ¥ì´ ì¢‹ì„ ë•Œ model_nì€ ì € ëª¨ë¸ë¥¼ ê°€ì ¸ì˜´\n",
    "        model_n = model_info.best_estimator_\n",
    "\n",
    "        #para:model_nì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë§Œ ì¶œë ¥\n",
    "        #model_nì—ì„œ ì˜ˆì‹œë¥¼ ë“¤ë©´ ì „ì²´ê°€ ì•„ë‹ˆë¼ ë’¤ì— ()ë‚´ìš©ë§Œ ì¶œë ¥\n",
    "        para=model_info.best_params_\n",
    "\n",
    "        #model_nì€ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ëª¨ë¸(í•˜ì´í¼íŒŒë¼ë¯¸í„°)ì¶œë ¥\n",
    "        #ì´ê²ƒì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        model_name = str(model_n)\n",
    "\n",
    "        #ëª¨ë¸ ì´ë¦„ì€ (ì•ì— ìˆìœ¼ë¯€ë¡œ (ë¡œ ìŠ¤í”Œë¦¿ ì´í›„ 0ë²ˆì§¸ ì¸ë±ìŠ¤\n",
    "        model_name = model_name.split('(')[0]\n",
    "\n",
    "       \n",
    "        model_n.fit(X_train,y_train)\n",
    "       \n",
    "\n",
    "        y_pred= model_n.predict(X_test)\n",
    "\n",
    "         #MAE,RMSE,R2,Adj-R2\n",
    "        mae=mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        R2 = r2_score(y_test,y_pred)\n",
    "        adj_r2 = adj_r2_score(y_test,y_pred,X_test)\n",
    "\n",
    "        #model Df  ì¤‘ê°„ ì˜¤ë¥˜ ë°œìƒìœ„í—˜ìœ¼ë¡œ y_test,y_predë¥¼ ì œì™¸í•˜ê³  í•˜ë‚˜ì”© ì¶”ê°€\n",
    "        model = pd.DataFrame({'y_test':y_test,'y_pred':y_pred})\n",
    "        model['MAE'] = mae\n",
    "        model['RMSE'] = rmse\n",
    "        model['R^2'] = R2\n",
    "        model['adj-R^2'] =adj_r2\n",
    "        model['Model'] = model_name\n",
    "        model['Training_time'] = end_time - start_time\n",
    "        model['Data_number']=Data_num\n",
    "        params=[]\n",
    "        #paraëŠ” ë”•ì…˜ë„ˆë¦¬ í˜•íƒœë¡œ ë˜ì–´ìˆìŒ\n",
    "        for param, value in para.items():\n",
    "            #ë”•ì…˜ë„ˆë¦¬ì¤‘ paramì€ ë³€ìˆ˜,value ê°’\n",
    "            params.append([param, value])\n",
    "        new_column_name = 'best_params'\n",
    "\n",
    "        #paramsë¥¼ ì¶œë ¥í•˜ê³  modelì˜ ê¸¸ì´ë§Œí¼ ë³µì‚¬\n",
    "        list_to_assign = [params] * len(model)\n",
    "\n",
    "        #ìƒˆë¡œìš´ ì—´ë¡œ ì¶”ê°€\n",
    "        model[new_column_name] = list_to_assign\n",
    "        if n == 0 :\n",
    "            result_model = model\n",
    "            n += 1\n",
    "        else:\n",
    "            result_model = pd.concat([result_model, model])\n",
    "            n += 1\n",
    "\n",
    "\n",
    " ####Stacking Code\n",
    "        #ë³€ìˆ˜ë¥¼ globalí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ ì–¸\n",
    "        globals()[model_name] = model_n\n",
    "\n",
    "    condition = (result_model['Data_number']==Data_num) #X1~X5-> Data_num\n",
    "    result_models=result_model[condition] #ìœ„ì— ì¡°ê±´ì— ë§ëŠ” DF\n",
    "    #adj-R^2ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    result_models=result_models.sort_values(by=[\"y_test\",'adj-R^2'], ascending=[True,False])\n",
    "    # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì˜ ì´ë¦„(ë¬¸ìì—´)->ë©”íƒ€ëª¨ë¸\n",
    "    meta_learner_name = result_models.iloc[0]['Model']\n",
    "    # ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "    final_meta_estimator = None\n",
    "    #ìœ„ì—ì„œ ì„ ì–¸í•œ í•¨ìˆ˜ì˜ ì´ë¦„ìœ¼ë¡œ ì„ ì–¸\n",
    "    meta_model_object = globals()[meta_learner_name]\n",
    "    final_meta_estimator = meta_model_object#RFë©´ meta_model_object=Rf_best_estimator_\n",
    "    print(\"Meta Learner for Stacking:\", final_meta_estimator) #meta ëª¨ë¸ ì¶œë ¥\n",
    "    length=len(models)\n",
    "    base_learner_names_series=result_models.iloc[1:length]['Model']\n",
    "    base_learner_names = base_learner_names_series.tolist()\n",
    "    # í›„ë³´ Base Learner ê°ì²´ ê°€ì ¸ì˜¤ê¸°\n",
    "    potential_base_objects = {}\n",
    "    valid_potential_base_names = [] # ì‹¤ì œë¡œ ê°ì²´ë¥¼ ì°¾ì€ ì´ë¦„ë§Œ ì €ì¥\n",
    "    for name in base_learner_names:\n",
    "        model_object = globals()[name]\n",
    "        actual_estimator = model_object#RFë©´ actual_estimator=Rf_best_estimator_\n",
    "        potential_base_objects[name] = actual_estimator\n",
    "        valid_potential_base_names.append(name)\n",
    "    #ëª¨ë“  ì¡°í•© ì‹œë„ ë° ìµœì ì¡°í•©\n",
    "    best_adj_R_2 = 0 #í°ê°’ ì°¾ì„ë• 0 ì‘ì€ê°’ ì°¾ì„ë• float('inf')\n",
    "    best_combination_names = None\n",
    "    results_log = []\n",
    "    min_base_learners = 2 #ìµœì†Œê°œìˆ˜(1ê°œë¡œ í• ê¹Œìš”?)\n",
    "    max_base_learners = min(6, len(valid_potential_base_names)) #6ê°œ ë˜ëŠ” ìµœì†Œê°’\n",
    "    if final_meta_estimator and len(valid_potential_base_names) >= min_base_learners:\n",
    "        start_combination_time = time.time()\n",
    "        total_combinations_tried = 0\n",
    "\n",
    "        for k in range(min_base_learners, max_base_learners + 1):\n",
    "            print(f\"\\n--- Trying combinations of size {k} ---\")\n",
    "            # í˜„ì¬ í¬ê¸° kì˜ ëª¨ë“  ì´ë¦„ ì¡°í•© ìƒì„±\n",
    "            for current_combination_names_tuple in itertools.combinations(valid_potential_base_names, k):\n",
    "                total_combinations_tried += 1\n",
    "                current_combination_names = list(current_combination_names_tuple)\n",
    "\n",
    "                # í˜„ì¬ ì¡°í•©ìœ¼ë¡œ estimators ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "                current_estimators = [(name, potential_base_objects[name]) for name in current_combination_names]\n",
    "\n",
    "                try:\n",
    "                    # Stacking Regressor ì •ì˜ ë° í•™ìŠµ\n",
    "                    ladder = StackingRegressor(estimators=current_estimators,\n",
    "                                            final_estimator=final_meta_estimator,\n",
    "                                            cv=5, # CVëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©ë¨\n",
    "                                            n_jobs=-1) # ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ê°€ëŠ¥\n",
    "\n",
    "                    ladder.fit(X_train, y_train) # í›ˆë ¨\n",
    "\n",
    "                    # ì˜ˆì¸¡ ë° MAE ê³„ì‚°\n",
    "                    y_pred_stack = ladder.predict(X_test)\n",
    "                    current_adj_R_2 = adj_r2_score(y_test, y_pred_stack, X_test) #<-R^2/adj_R^2/RMSE/MSEë¡œ ë³€ê²½ê°€ëŠ¥(metrics ë³€ê²½ì‹œ) \n",
    "\n",
    "                    # ê²°ê³¼ ë¡œê¹…\n",
    "                    results_log.append({'combination': current_combination_names, 'adj_R^2': current_adj_R_2}) #metrics ë³€ê²½ì‹œ ìˆ˜ì • \n",
    "                    # print(f\"  Combination: {current_combination_names}, MAE: {current_mae:.4f}\") # ìƒì„¸ ë¡œê·¸\n",
    "\n",
    "                    # ìµœê³  ê¸°ë¡ ì—…ë°ì´íŠ¸\n",
    "                    if current_adj_R_2 > best_adj_R_2:#metrics ë³€ê²½ì‹œ ìˆ˜ì •  #í°ê°’ ì°¾ì„ë• > ì‘ì€ê°’ ì°¾ì„ë• <\n",
    "                        best_adj_R_2 = current_adj_R_2#metrics ë³€ê²½ì‹œ ìˆ˜ì • \n",
    "                        best_combination_names = current_combination_names\n",
    "                        print(f\" ğŸ¯ >>> New Best adj_R^2: {best_adj_R_2:.2f} with combination: {best_combination_names}\") #metrics ë³€ê²½ì‹œ ìˆ˜ì • \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during stacking with combination {current_combination_names}: {e}\")\n",
    "\n",
    "        end_combination_time = time.time()\n",
    "        print(f\"\\n--- Combination Search Finished ---\")\n",
    "        print(f\"Total combinations tried: {total_combinations_tried}\")\n",
    "        print(f\"Search Time: {end_combination_time - start_combination_time:.2f} seconds\")\n",
    "\n",
    "        if best_combination_names:\n",
    "            print(f\"Best adj_R^2 found: {best_adj_R_2:.2f}\") #metrics ë³€ê²½ì‹œ ìˆ˜ì • \n",
    "            print(f\"Best Base Learner Combination: {best_combination_names}\")\n",
    "\n",
    "            # ìµœì  ì¡°í•©ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ êµ¬ì„± (í•„ìš”ì‹œ)\n",
    "            final_best_estimators = [(name, potential_base_objects[name]) for name in best_combination_names]\n",
    "            # ì´ final_best_estimators ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ ë˜ëŠ” ì €ì¥ ë“±ì„ ìˆ˜í–‰\n",
    "    else:\n",
    "        print(\"Could not find a best combination (possibly due to errors or no combinations tested).\")\n",
    "\n",
    "    #ìµœì ì˜ ì¡°í•©ì„ ì´ìš©í•˜ì—¬ ë°ì´í„° ì˜ˆì¸¡\n",
    "    meta_name=['Meta',meta_learner_name]\n",
    "    #base ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    base_name=[]\n",
    "\n",
    "    for i in final_best_estimators:\n",
    "        #i[1]-> í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "        base= i[0]\n",
    "        base_name.append(base)\n",
    "\n",
    "    print(\"Base Learners for Stacking:\", base_name)\n",
    "    #ìœ„ì—ì„œ ì¶œë ¥í•œ final_best_estimatorë¡œ í›ˆë ¨\n",
    "    ladder = StackingRegressor(estimators=final_best_estimators,\n",
    "                        final_estimator=final_meta_estimator,\n",
    "                        cv=5)\n",
    "    ladder.fit(X_train, y_train)\n",
    "\n",
    "    #ì˜ˆì¸¡\n",
    "    y_pred= ladder.predict(X_test)\n",
    "\n",
    "    #í”¼ì²˜ ì¤‘ìš”ë„ \n",
    "    result = permutation_importance(\n",
    "    ladder, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1, scoring='r2' # ì˜ˆì‹œ: íšŒê·€, R2 ê¸°ì¤€\n",
    "    )\n",
    "    perm_sorted_idx = result.importances_mean.argsort() # ì¤‘ìš”ë„ í‰ê·  ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì¸ë±ìŠ¤\n",
    "    feature_names = X_test.columns if isinstance(X_test, pd.DataFrame) else [f'Feature {i}' for i in range(X_test.shape[1])]\n",
    "    perm_importance_data = {\n",
    "        'Feature': feature_names,\n",
    "        'Mean_Importance': result.importances_mean,\n",
    "        'Std_Importance': result.importances_std,\n",
    "        'Data_number': Data_num\n",
    "        }\n",
    "    perm_importance_df = pd.DataFrame(perm_importance_data)\n",
    "    if num==0:\n",
    "        perm_importance_df_all = perm_importance_df\n",
    "    else:\n",
    "        perm_importance_df_all = pd.concat([perm_importance_df_all, perm_importance_df])\n",
    "    # ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "    perm_importance_df = perm_importance_df.sort_values(\n",
    "        by='Mean_Importance', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n--- Permutation Importance (Original Features) DataFrame ---\")\n",
    "    print(perm_importance_df)\n",
    "    #MAE,RMSE,R2,Adj-R2\n",
    "    mae = mean_absolute_error(y_test,y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    R2 = r2_score(y_test,y_pred)\n",
    "    adj_r2 = adj_r2_score(y_test,y_pred,X_test)\n",
    "\n",
    "    #Stacking Df ì œì‘ ë° ì¤‘ê°„ ì˜¤ë¥˜ ë°œìƒìœ„í—˜ìœ¼ë¡œ y_test,y_predë¥¼ ì œì™¸í•˜ê³  í•˜ë‚˜ì”© ì¶”ê°€\n",
    "    stacking = pd.DataFrame({'y_test':y_test,'y_pred':y_pred})\n",
    "    stacking['MAE'] = mae\n",
    "    stacking['RMSE'] = rmse\n",
    "    stacking['R^2'] = R2\n",
    "    stacking['adj-R^2'] = adj_r2\n",
    "    stacking['Model'] = 'Stacking'\n",
    "    stacking['Training_time'] = 0\n",
    "    #Data_number -> X1~X5\n",
    "    stacking['Data_number']=Data_num\n",
    "    #meta_model_name,base_model_name\n",
    "    params=[meta_name,base_name]\n",
    "    print(params)\n",
    "\n",
    "    #paramsë¥¼ stacking í¬ê¸°ë§Œí¼ ë³µì‚¬\n",
    "    list_to_base_name = [params] * len(stacking)\n",
    "    stacking['best_params'] = list_to_base_name\n",
    "\n",
    "    if num == 0:\n",
    "        result_stack = stacking\n",
    "    else:\n",
    "        result_stack = pd.concat([result_stack, stacking])\n",
    "\n",
    "    num+=1\n",
    "#Model_stackìœ¼ë¡œ stackingê³¼ model concat\n",
    "Model_stack = pd.concat([result_model, result_stack])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
